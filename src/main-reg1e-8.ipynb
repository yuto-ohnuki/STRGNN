{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json, copy, glob, random, datetime\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import CosineEmbeddingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GAE\n",
    "from torch_geometric.nn.conv import GCNConv, RGCNConv, MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from networkx.drawing.layout import bipartite_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_path(pth, depth=1):\n",
    "    for _ in range(depth):\n",
    "        pth = os.path.dirname(pth)\n",
    "    return pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_files(pth):\n",
    "    for f in glob.glob(os.path.join(pth, '*')):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(arr):\n",
    "    return set([(l[0], l[1]) for l in arr.T.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_set(s1,s2):\n",
    "    return s1.union(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(txt, time_begin, time_fin):\n",
    "    time = time_fin - time_begin\n",
    "    print(\"{}: {:0.3f}\".format(txt, time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_feat(num_nodes, dim):\n",
    "    x = torch.Tensor(num_nodes, dim)\n",
    "    x = x.data.normal_(std=1/np.sqrt(dim))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_ids(var):\n",
    "    if isinstance(var, pd.DataFrame):\n",
    "        ret = len(var['MyID'].unique())\n",
    "        return ret\n",
    "    else:\n",
    "        try:\n",
    "            ret = len(set([x['id'] for x in var.values()]))\n",
    "            return ret\n",
    "        except:\n",
    "            raise Exception(\"Dtype error: Node file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_undirected(edge_index, n_src, edge_type=None, is_bipartite=True):\n",
    "    if is_bipartite:\n",
    "        edge_index[1] += n_src\n",
    "    rev_edge_index = edge_index.clone()\n",
    "    rev_edge_index[0,:], rev_edge_index[1,:] = edge_index[1,:], edge_index[0,:]\n",
    "\n",
    "    if edge_type is None:\n",
    "        return torch.cat([edge_index, rev_edge_index], dim=1)\n",
    "    else:\n",
    "        return torch.cat([edge_index, rev_edge_index], dim=1), torch.cat([edge_type, edge_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nodes(path, keys):\n",
    "    # load node dataframes\n",
    "    nodes = dict()\n",
    "    for key in keys:\n",
    "        filepath = os.path.join(path, \"Node\", \"{}.csv\".format(key))\n",
    "        if os.path.exists(filepath):\n",
    "            nodes[key] = pd.read_csv(filepath)\n",
    "        else:\n",
    "            print(\"{} node file does not exist\".format(key))\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edges(path, unweighted_keys, weighted_keys):\n",
    "    edge_indexes = dict()\n",
    "    edge_weights = dict()\n",
    "    \n",
    "    # load network data (unweighted)\n",
    "    for key in unweighted_keys:\n",
    "        filepath = os.path.join(path, 'Edge', \"{}.npy\".format(key))\n",
    "        if os.path.exists(filepath):\n",
    "            edge = sp.coo_matrix(np.load(filepath))\n",
    "            edge = Tensor(np.array([edge.row, edge.col])).long()\n",
    "            edge_indexes[key] = edge\n",
    "        else:\n",
    "            print(\"{} network file does not exist\".format(key))\n",
    "    \n",
    "    # load network data (weighted)\n",
    "    for key in weighted_keys:\n",
    "        filepath = os.path.join(path, 'Edge', \"{}.npy\".format(key))\n",
    "        if os.path.exists(filepath):\n",
    "            edge = np.load(filepath)\n",
    "            row, col = edge.nonzero()\n",
    "            edge_indexes[key] = Tensor([row, col]).long()\n",
    "            edge_weights[key] = Tensor(np.array([edge[i][j] for i,j in zip(row, col)]))\n",
    "        else:\n",
    "            print(\"{} network file does not exist\".format(key))\n",
    "    \n",
    "    return edge_indexes, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embs(nodes, keys, nums, MXLEN=1000):\n",
    "    embs = dict()\n",
    "    drug_dim = 2048\n",
    "    prot_dim = MXLEN\n",
    "    \n",
    "    # Drug\n",
    "    if 'drug_ecfp' in keys:\n",
    "        mat = np.zeros((nums['drug'], drug_dim))\n",
    "        for myid in tqdm(range(nums['drug'])):\n",
    "            sm = nodes['drug'][nodes['drug'].MyID==myid]\n",
    "            vec = sm['ECFP'].iloc[0][1:-1].replace(',','').split()\n",
    "            for x in vec:\n",
    "                mat[myid][int(x)] = 1\n",
    "        embs['drug_ecfp'] = mat\n",
    "    \n",
    "    # Protein\n",
    "    if 'protein_seq' in keys:\n",
    "        seq = nodes['protein']['Sequence'].values\n",
    "        uniq_chars = set()\n",
    "        char_to_id = {}\n",
    "        for line in seq:\n",
    "            uniq_chars = uniq_chars.union(set(list(line)))\n",
    "        for e,x in enumerate(list(uniq_chars)):\n",
    "            char_to_id[x] = e\n",
    "        mat = np.empty((len(seq), len(char_to_id), prot_dim))\n",
    "        for i,s in enumerate(tqdm(seq)):\n",
    "            if len(s) > MXLEN:\n",
    "                s = s[:MXLEN]\n",
    "            for j,c in enumerate(s):\n",
    "                mat[i][char_to_id[c]][j] = 1\n",
    "        embs['protein_seq'] = mat\n",
    "    \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataset(data, nodes, edges, edge_symbols, conf):\n",
    "    \n",
    "    line = \"#\"*40\n",
    "    src_node, tar_node = conf['target_network'].split('_')\n",
    "    print(line)\n",
    "    \n",
    "    # Task type\n",
    "    task_type = conf['task_type']\n",
    "    print(\"Task  : {}\".format(task_type))\n",
    "    print(\"Target: {}\".format(conf['target_network']))\n",
    "    print(line)\n",
    "    \n",
    "    # Dataset\n",
    "    print('Node counts >>')\n",
    "    for key in nodes:\n",
    "        print('\\t{}: {}'.format(key, data['n_{}'.format(key)]))\n",
    "\n",
    "    print(\"\\nEdge counts >>\")\n",
    "    for key in edges:\n",
    "        src, tar, *diff = key.split('_')\n",
    "        if src==tar:\n",
    "            print('\\t{}: {}'.format(key, data[\"{}_edge_index\".format(edge_symbols[key])].shape[1]))\n",
    "        else:\n",
    "            print('\\t{}: {}'.format(key, data[\"{}_edge_index\".format(edge_symbols[key])].shape[1]//2))\n",
    "    print(line)\n",
    "    \n",
    "    # Link prediction\n",
    "    print(\"Node splits >> \")\n",
    "    print(\"\\tInternal {} nodes: {}\".format(src_node, data.internal_src_index.shape[0]))\n",
    "    print(\"\\tInternal {} nodes: {}\\n\".format(tar_node, data.internal_tar_index.shape[0]))\n",
    "    print(\"\\tExternal {} nodes: {}\".format(src_node, data.external_src_index.shape[0]))\n",
    "    print(\"\\tExternal {} nodes: {}\".format(tar_node, data.external_tar_index.shape[0]))\n",
    "    print(line)\n",
    "    \n",
    "    \n",
    "    # Train, Valid, Test\n",
    "    print(\"Edge splits >> \")\n",
    "    for i in range(conf['cv']):\n",
    "        print(\"\\tTrain (cv-{}): {} >> {}\".format(i, \"INT\", data.train_edge_index[i].shape[1]))\n",
    "        print(\"\\tValid (cv-{}): {} >> {}\\n\".format(i, \"INT\" if task_type=='transductive' else \"EXT\", data.valid_edge_index[i].shape[1]))\n",
    "    print(\"\\tTest: {} >> {}\".format(\"INT\" if task_type=='transductive' else \"EXT\", data.test_edge_index.shape[1]))\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partitioning considering drug node order and ICD-11 classification \n",
    "def DrugRepositioning_edge_split(dz_edge_index, diz_df):\n",
    "    train_edge_index = []\n",
    "    test_edge_index = []\n",
    "    \n",
    "    # drug - disease pair\n",
    "    dz_pairs = defaultdict(list)\n",
    "    for src, tar in dz_edge_index.T:\n",
    "        src = src.item()\n",
    "        tar = tar.item()\n",
    "        dz_pairs[src].append(tar)\n",
    "    \n",
    "    # Drugs: Degree >= 2\n",
    "    drugs = dz_edge_index[0].tolist()\n",
    "    drug_src_counts = Counter(drugs)\n",
    "    test_drug_id = [k for k,v in drug_src_counts.items() if v>=2]\n",
    "    \n",
    "    # Disease: No overlapping ICD-11 classifications\n",
    "    for did in test_drug_id:\n",
    "        tars = dz_pairs[did]\n",
    "        tmp = diz_df[diz_df['MyID'].isin(tars)]\n",
    "        diz_cats = tmp['ICD-11_Category'].values\n",
    "\n",
    "        if len(set(diz_cats)) <= 1:\n",
    "            continue\n",
    "        else:\n",
    "            picked_cat = np.random.choice(diz_cats)\n",
    "            picked_diz = tmp[tmp['ICD-11_Category']==picked_cat]\n",
    "            for zid in picked_diz.MyID:\n",
    "                test_edge_index.append([did, zid])\n",
    "            \n",
    "    # split train-test\n",
    "    test_set = set([(x,y) for x,y in test_edge_index])\n",
    "    for x,y in dz_edge_index.T:\n",
    "        x = x.item()\n",
    "        y = y.item()\n",
    "        if (x,y) not in test_set:\n",
    "            train_edge_index.append([x,y])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    train_edge_index = Tensor(np.array(train_edge_index)).T.long()\n",
    "    test_edge_index = Tensor(np.array(test_edge_index)).T.long()\n",
    "    return train_edge_index, test_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partitioning without considering drug repositioning\n",
    "def DrugProtein_edge_split(dp_edge_index, train_ratio):\n",
    "    train_edge_index = []\n",
    "    test_edge_index = []\n",
    "    n_edge = dp_edge_index.shape[1]\n",
    "    \n",
    "    rd = np.random.binomial(1, train_ratio, n_edge)\n",
    "    train_mask = rd.nonzero()[0]\n",
    "    test_mask = (1-rd).nonzero()[0]\n",
    "    \n",
    "    train_index = dp_edge_index[:, train_mask]\n",
    "    test_index = dp_edge_index[:, test_mask]\n",
    "    \n",
    "    train_edge_index = Tensor(np.array(train_index)).long()\n",
    "    test_edge_index = Tensor(np.array(test_index)).long()\n",
    "    \n",
    "    return train_edge_index, test_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_edges(edge_index, train_ratio=0.8, cv=1):\n",
    "    n_edge = edge_index.shape[1]\n",
    "    ret_train, ret_valid = [], []\n",
    "    \n",
    "    if cv==1:\n",
    "        rd = np.random.binomial(1, train_ratio, n_edge)\n",
    "        train_mask = rd.nonzero()[0]\n",
    "        valid_mask = (1-rd).nonzero()[0]\n",
    "        \n",
    "        train_index = edge_index[:, train_mask]\n",
    "        valid_index = edge_index[:, valid_mask]\n",
    "        \n",
    "        ret_train.append(train_index.long())\n",
    "        ret_valid.append(valid_index.long())\n",
    "    \n",
    "    else:\n",
    "        rd = np.random.permutation(np.arange(n_edge)%cv)\n",
    "        for i in range(cv):\n",
    "            train_index = edge_index[:, rd!=i]\n",
    "            valid_index = edge_index[:, rd==i]\n",
    "            ret_train.append(train_index.long())\n",
    "            ret_valid.append(valid_index.long())\n",
    "    \n",
    "    return ret_train, ret_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inductive_edges(edge_index, split_ratio=0.5):\n",
    "    n_edge = edge_index.shape[1]\n",
    "    ret_valid = []\n",
    "\n",
    "    rd = np.random.binomial(1, split_ratio, n_edge)\n",
    "    valid_mask = rd.nonzero()[0]\n",
    "    test_mask = (1-rd).nonzero()[0]\n",
    "\n",
    "    valid_index = [edge_index[:, valid_mask]]\n",
    "    test_index = edge_index[:, test_mask]\n",
    "    \n",
    "    return valid_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_edge_from_mask(edge_index, edge_weight=None, src_mask_index=None, tar_mask_index=None):\n",
    "    ret_index, ret_weight = [], []\n",
    "    is_mask_src = (src_mask_index is not None)\n",
    "    is_mask_tar = (tar_mask_index is not None)\n",
    "    \n",
    "    # unweighted edge index\n",
    "    if edge_weight is None:\n",
    "        for pair in edge_index.T.numpy():\n",
    "            flg = True\n",
    "            if is_mask_src and pair[0] in src_mask_index:\n",
    "                flg = False\n",
    "            if is_mask_tar and pair[1] in tar_mask_index:\n",
    "                flg = False\n",
    "            if flg:\n",
    "                ret_index.append(pair)\n",
    "        ret_index = Tensor(np.array(ret_index).T).long()\n",
    "        return ret_index\n",
    "    \n",
    "    # weighted edge index\n",
    "    else:\n",
    "        for pair, weight in zip(edge_index.T.numpy(), edge_weight.T):\n",
    "            flg = True\n",
    "            if is_mask_src and pair[0] in src_mask_index:\n",
    "                flg = False\n",
    "            if is_mask_tar and pair[1] in tar_mask_index:\n",
    "                flg = False\n",
    "            if flg:\n",
    "                ret_index.append(pair)\n",
    "                ret_weight.append(weight)\n",
    "        ret_index = Tensor(np.array(ret_index).T).long()\n",
    "        ret_weight = Tensor(np.array(ret_weight))\n",
    "        return ret_index, ret_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_edge_split(data, edge_symbols, weighted_edge_names, unseen_ratio, conf):\n",
    "    \n",
    "    source_node = conf['source_node']\n",
    "    target_node = conf['target_node']\n",
    "    \n",
    "    # Semi inductive\n",
    "    if conf['task_type']=='semi_inductive':\n",
    "        network = data['{}_edge_index'.format(edge_symbols[conf['target_network']])]\n",
    "        src_dims = Counter(network[0].tolist())\n",
    "        cand_src_ids = np.array([k for k,v in src_dims.items() if v>=2])\n",
    "        rd = np.random.binomial(1, unseen_ratio, len(cand_src_ids))\n",
    "        external_src_index = cand_src_ids[rd.nonzero()[0]]\n",
    "        external_src_set = set(external_src_index)\n",
    "        internal_src_index = np.array([x for x in range(data['n_{}'.format(source_node)]) if x not in external_src_set])\n",
    "        internal_src_set = set(internal_src_index)\n",
    "        \n",
    "        internal_tar_index = np.array([x for x in range(data['n_{}'.format(target_node)])])\n",
    "        internal_tar_set = set(internal_tar_index)\n",
    "        external_tar_index = np.array([])\n",
    "        external_tar_set = set(external_tar_index)\n",
    "    \n",
    "    # Full inductive\n",
    "    elif conf['task_type']=='full_inductive':\n",
    "        network = data['{}_edge_index'.format(edge_symbols[conf['target_network']])]\n",
    "        src_dims = Counter(network[0].tolist())\n",
    "        tar_dims = Counter(network[1].tolist())\n",
    "        cand_src_ids = np.array([k for k,v in src_dims.items() if v>=2])\n",
    "        cand_tar_ids = np.array([k for k,v in tar_dims.items() if v>=2])\n",
    "        rd_src = np.random.binomial(1, unseen_ratio, len(cand_src_ids))\n",
    "        rd_tar = np.random.binomial(1, unseen_ratio, len(cand_tar_ids))\n",
    "        \n",
    "        external_src_index = cand_src_ids[rd_src.nonzero()[0]]\n",
    "        external_src_set = set(external_src_index)\n",
    "        external_tar_index = cand_tar_ids[rd_tar.nonzero()[0]]\n",
    "        external_tar_set = set(external_tar_index)\n",
    "        \n",
    "        internal_src_index = np.array([x for x in range(data.n_drug) if x not in external_src_set])\n",
    "        internal_src_set = set(internal_src_index)\n",
    "        internal_tar_index = np.array([x for x in range(data.n_protein) if x not in external_tar_set])\n",
    "        internal_tar_set = set(internal_tar_index)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Task type error\")\n",
    "    \n",
    "    # train-test split for target network\n",
    "    train_edge_index, test_edge_index = [], []\n",
    "    for pair in data['{}_edge_index'.format(edge_symbols[conf['target_network']])].T.numpy():\n",
    "        if pair[0] in internal_src_set:\n",
    "            if conf['task_type']=='semi_inductive':\n",
    "                train_edge_index.append(pair)\n",
    "            else:\n",
    "                if pair[1] in internal_tar_set:\n",
    "                    train_edge_index.append(pair)\n",
    "        elif pair[0] in external_src_set:\n",
    "            if conf['task_type']=='semi_inductive':\n",
    "                test_edge_index.append(pair)\n",
    "            else:\n",
    "                if pair[1] in external_tar_set:\n",
    "                    test_edge_index.append(pair)\n",
    "        else:\n",
    "            continue\n",
    "    train_edge_index = np.array(train_edge_index)\n",
    "    test_edge_index = np.array(test_edge_index)\n",
    "    \n",
    "    # remove edges including external source nodes\n",
    "    updated_edge_index = {}\n",
    "    updated_edge_weight = {}\n",
    "    mask_index = {\n",
    "        source_node: external_src_index,\n",
    "        target_node: external_tar_index\n",
    "    }\n",
    "    for network, symbol in edge_symbols.items():\n",
    "        types = network.split('_')\n",
    "        if network==conf['target_network']:\n",
    "            updated_edge_index[symbol] = Tensor(train_edge_index).T.long()\n",
    "        else:\n",
    "            first, second = types[0], types[1]\n",
    "            if first in mask_index.keys():\n",
    "                first_mask = mask_index[first]\n",
    "            else:\n",
    "                first_mask = None\n",
    "            if second in mask_index.keys():\n",
    "                second_mask = mask_index[second]\n",
    "            else:\n",
    "                second_mask = None\n",
    "            \n",
    "            if (first_mask is None) and (second_mask is None):\n",
    "                updated_edge_index[symbol] = data['{}_edge_index'.format(symbol)]\n",
    "                if network in weighted_edge_names:\n",
    "                    updated_edge_weight[symbol] = data['{}_edge_weight'.format(symbol)]\n",
    "            else:\n",
    "                if network in weighted_edge_names:\n",
    "                    updated_edge_index[symbol], updated_edge_weight[symbol] = remove_edge_from_mask(\n",
    "                        data['{}_edge_index'.format(symbol)], data['{}_edge_weight'.format(symbol)],\n",
    "                        src_mask_index=first_mask, tar_mask_index=second_mask\n",
    "                    )\n",
    "                else:\n",
    "                    updated_edge_index[symbol] = remove_edge_from_mask(\n",
    "                        data['{}_edge_index'.format(symbol)],\n",
    "                        src_mask_index=first_mask, tar_mask_index=second_mask\n",
    "                    )\n",
    "    # update edge_index and edge_weight\n",
    "    for key, val in updated_edge_index.items():\n",
    "        data['{}_edge_index'.format(key)] = val\n",
    "    for key, val in updated_edge_weight.items():\n",
    "        data['{}_edge_weight'.format(key)] = val\n",
    "    \n",
    "    # save as data\n",
    "    data.internal_src_index = internal_src_index\n",
    "    data.internal_tar_index = internal_tar_index\n",
    "    data.external_src_index = external_src_index\n",
    "    data.external_tar_index = external_tar_index\n",
    "    \n",
    "    train_edge_index = [Tensor(train_edge_index.T).long()]\n",
    "    test_edge_index = Tensor(test_edge_index.T).long()\n",
    "    return data, train_edge_index, test_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_link_prediction_datas(data, nodes, edge_symbols, weighted_edges, conf, unseen_ratio=0.5):\n",
    "    src_node, tar_node = conf['target_network'].split('_')\n",
    "    if conf['task_type']=='transductive':\n",
    "        print(\"Transductive link prediction\")\n",
    "        if conf['target_network'] == 'drug_disease':\n",
    "            train_edge_index, test_edge_index = DrugRepositioning_edge_split(\n",
    "                data.d_z_edge_index, nodes['disease']\n",
    "            )\n",
    "            train_edge_index, valid_edge_index = split_train_valid_edges(train_edge_index, train_ratio=0.8, cv=conf['cv'])\n",
    "        \n",
    "        elif conf['target_network'] == 'drug_protein':\n",
    "            train_edge_index, test_edge_index = DrugProtein_edge_split(\n",
    "                data.d_p_edge_index, train_ratio=0.8\n",
    "            )\n",
    "            train_edge_index, valid_edge_index = split_train_valid_edges(train_edge_index, train_ratio=0.8, cv=conf['cv'])\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(\"Target network is not considered\")\n",
    "\n",
    "            \n",
    "        data.internal_src_index = np.array([i for i in range(data['n_{}'.format(src_node)])])\n",
    "        data.external_src_index = np.array([])\n",
    "        data.internal_tar_index = np.array([i for i in range(data['n_{}'.format(tar_node)])])\n",
    "        data.external_tar_index = np.array([])\n",
    "        data.train_edge_index = train_edge_index\n",
    "        data.valid_edge_index = valid_edge_index\n",
    "        data.test_edge_index = test_edge_index\n",
    "        return data\n",
    "        \n",
    "        \n",
    "    elif conf['task_type']=='semi_inductive' or conf['task_type']=='full_inductive':\n",
    "        print(\"Inductive link prediction\")\n",
    "        data, train_edge_index, test_edge_index = unseen_edge_split(\n",
    "            data, edge_symbols, weighted_edges, unseen_ratio=unseen_ratio, conf=conf\n",
    "        )\n",
    "        valid_edge_index, test_edge_index = split_inductive_edges(test_edge_index, split_ratio=0.5)\n",
    "        \n",
    "        data.train_edge_index = train_edge_index\n",
    "        data.valid_edge_index = valid_edge_index\n",
    "        data.test_edge_index = test_edge_index\n",
    "        return data\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Task type error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling edges\n",
    "def negative_sampling_edge_index(pos_edge_index, n_src, n_tar, used, current_src_index, current_tar_index, conf):\n",
    "    \"\"\"\n",
    "        - pos_edge_index: positive samples\n",
    "        - n_src, n_tar: number of nodes\n",
    "        - used: used pair for positive samples\n",
    "        - current_src_index, current_tar_index: node indexes for current task\n",
    "            - Transductive task\n",
    "                - training -> internal node index\n",
    "                - valid, test -> internal node index\n",
    "            - Inductive task\n",
    "                - training -> internal node index\n",
    "                - valid, test -> external node index\n",
    "        - conf: configuration\n",
    "    \"\"\"\n",
    "    pos = pos_edge_index.clone().cpu().numpy()\n",
    "    pos_pair = set([(l[0], l[1]) for l in pos.T.tolist()])\n",
    "    current_src_set = set(current_src_index)\n",
    "    current_tar_set = set(current_tar_index)\n",
    "    neg_srcs = [0]*pos.shape[1]\n",
    "    neg_tars = [0]*pos.shape[1]\n",
    "    \n",
    "    \"\"\" Transductive, Fully-Inductive \"\"\"\n",
    "    if conf['task_type']=='transductive' or conf['task_type']=='full_inductive':\n",
    "        \n",
    "        # choice random source nodes\n",
    "        for i, tar in enumerate(pos[1]):\n",
    "            while True:\n",
    "                neg_src = np.random.randint(0, n_src)\n",
    "                if (used is not None) and ({(neg_src, tar)} <= used):\n",
    "                    continue\n",
    "                if neg_src not in current_src_set:\n",
    "                    continue\n",
    "                if not {(neg_src, tar)} <= pos_pair:\n",
    "                    break\n",
    "            neg_srcs[i] = neg_src\n",
    "        \n",
    "        # choice random target nodes\n",
    "        for i, src in enumerate(pos[0]):\n",
    "            while True:\n",
    "                neg_tar = np.random.randint(0, n_tar)\n",
    "                if (used is not None) and ({(src, neg_tar)} <= used):\n",
    "                    continue\n",
    "                if neg_tar not in current_tar_set:\n",
    "                    continue\n",
    "                if not {(src, neg_tar)} <= pos_pair:\n",
    "                    break\n",
    "            neg_tars[i] = neg_tar\n",
    "        \n",
    "        neg_edge = Tensor(np.concatenate([np.array([pos[0], neg_tars]), np.array([neg_srcs, pos[1]])], axis=1)).long()\n",
    "        return neg_edge\n",
    "    \n",
    "    \"\"\" Semi - Inductive (simple) \"\"\"\n",
    "    if conf['task_type']=='semi_inductive' and conf['negative_style']=='simple':\n",
    "        neg_edge = []\n",
    "        negative_size = 2\n",
    "        \n",
    "        # only choice random target nodes\n",
    "        for i, src in enumerate(pos[0]):\n",
    "            for _ in range(negative_size):\n",
    "                while True:\n",
    "                    neg_tar = np.random.randint(0, n_tar)\n",
    "                    if (used is not None) and ({(src, neg_tar)} <= used):\n",
    "                        continue\n",
    "                    if not {(src, neg_tar)} <= pos_pair:\n",
    "                        break\n",
    "                neg_edge.append([src, neg_tar])\n",
    "        neg_edge = Tensor(np.array(neg_edge)).T.long()\n",
    "        return neg_edge\n",
    "    \n",
    "    \n",
    "    \"\"\" Semi - Inductive (hard) \"\"\"\n",
    "    if conf['task_type']=='semi_inductive' and conf['negative_style']=='hard':\n",
    "        \n",
    "        # choice random source nodes\n",
    "        for i, tar in enumerate(pos[1]):\n",
    "            while True:\n",
    "                neg_src = np.random.randint(0, n_src)\n",
    "                if (used is not None) and ({(neg_src, tar)} <= used):\n",
    "                    continue\n",
    "                if neg_src not in current_src_set:\n",
    "                    continue\n",
    "                if not {(neg_src, tar)} <= pos_pair:\n",
    "                    break\n",
    "            neg_srcs[i] = neg_src\n",
    "        \n",
    "        # choice random target nodes\n",
    "        for i, src in enumerate(pos[0]):\n",
    "            while True:\n",
    "                neg_tar = np.random.randint(0, n_tar)\n",
    "                if (used is not None) and ({(src, neg_tar)} <= used):\n",
    "                    continue\n",
    "                if neg_tar not in current_tar_set:\n",
    "                    continue\n",
    "                if not {(src, neg_tar)} <= pos_pair:\n",
    "                    break\n",
    "            neg_tars[i] = neg_tar\n",
    "        \n",
    "        neg_edge = Tensor(np.concatenate([np.array([pos[0], neg_tars]), np.array([neg_srcs, pos[1]])], axis=1)).long()\n",
    "        return neg_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(data, ret_feat, test_edge_index, test_neg_edge_index, conf):\n",
    "    \n",
    "    model.eval()\n",
    "    if conf['task_type'] == 'inductive':\n",
    "        x_drug = data.drug_ecfp\n",
    "        z_drug = model.encoder.att_encoder['ecfp'](x_drug).to(conf['device'])\n",
    "        z_diz = ret_feat['z_feat']\n",
    "    elif conf['task_type'] == 'transductive':\n",
    "        z_drug = ret_feat['d_feat']\n",
    "        z_diz = ret_feat['z_feat']\n",
    "    \n",
    "    pos_score = model.decoder(z_drug, z_diz, test_edge_index)\n",
    "    neg_score = model.decoder(z_drug, z_diz, test_neg_edge_index)\n",
    "    \n",
    "    pos_target = torch.ones(pos_score.shape[0])\n",
    "    neg_target = torch.zeros(neg_score.shape[0])\n",
    "    \n",
    "    score = torch.cat([pos_score, neg_score])\n",
    "    target = torch.cat([pos_target, neg_target])\n",
    "    \n",
    "    \n",
    "    pred = (score.detach().cpu().numpy().flatten() >= 0.5).astype(int)\n",
    "    label = target.detach().cpu().numpy()\n",
    "    \n",
    "    cm = confusion_matrix(pred, label)\n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all node's embeddings by PCA\n",
    "def plot_global_PCA(feats, nsymbols, figname=None):\n",
    "    # concat feature\n",
    "    for e, (key, value) in enumerate(nsymbols.items()):\n",
    "        tmp = feats['{}_feat'.format(value)]\n",
    "        if e==0:\n",
    "            vec = tmp\n",
    "        else:\n",
    "            vec = torch.cat((vec, tmp), 0)\n",
    "    pca = PCA(n_components=2)\n",
    "    ret = pca.fit_transform(vec.detach().cpu())\n",
    "    \n",
    "    # plot\n",
    "    idx = 0\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    labels = list(nsymbols.keys())\n",
    "    for e, (network, symbol) in enumerate(nsymbols.items()):\n",
    "        tmp = ret[idx:idx+data['n_{}'.format(network)]]\n",
    "        plt.scatter(tmp[:,0], tmp[:,1], label=labels[e], alpha=0.6)\n",
    "        idx += data['n_{}'.format(network)]\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=15)\n",
    "    plt.xlabel('PC1: {:.3f}'.format(pca.explained_variance_ratio_[0]))\n",
    "    plt.ylabel('PC2: {:.3f}'.format(pca.explained_variance_ratio_[1]))\n",
    "    if figname!=None:\n",
    "        if not os.path.isdir('global_PCA'):\n",
    "            os.mkdir('global_PCA')\n",
    "        fig.savefig(os.path.join('global_PCA', figname), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_local_PCA(feat, node_df, labeldic, figname=None):\n",
    "    if type(feat).__module__=='numpy':\n",
    "        pass\n",
    "    elif type(feat).__module__=='torch':\n",
    "        feat = feat.detach().cpu()\n",
    "    else:\n",
    "        raise Exception(\"Embedding datatype error\")\n",
    "    pca = PCA(n_components=2)\n",
    "    ret = pca.fit_transform(feat)\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    for cls in list(set(labeldic.values())):\n",
    "        idx = node_df[node_df.ecfp_cluster==cls].MyID.values\n",
    "        idx = list(set(idx))\n",
    "        tmp = ret[idx,:]\n",
    "        plt.scatter(tmp[:,0], tmp[:,1], label=cls, alpha=0.6)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=15)\n",
    "    plt.xlabel('PC1: {:.3f}'.format(pca.explained_variance_ratio_[0]))\n",
    "    plt.ylabel('PC2: {:.3f}'.format(pca.explained_variance_ratio_[1]))\n",
    "    plt.title(\"PCA\")\n",
    "    if figname!=None:\n",
    "        if not os.path.isdir('local_PCA'):\n",
    "            os.mkdir('local_PCA')\n",
    "        fig.savefig(os.path.join('local_PCA', figname), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_local_TSNE(feat, node_df, labeldic, savefig=False, figname=None):\n",
    "    if type(feat).__module__=='numpy':\n",
    "        pass\n",
    "    elif type(feat).__module__=='torch':\n",
    "        feat = feat.detach().cpu()\n",
    "    else:\n",
    "        raise Exception(\"Embedding datatype error\")\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=1000)\n",
    "    ret = tsne.fit_transform(feat)\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    for cls in list(set(labeldic.values())):\n",
    "        idx = node_df[node_df.ecfp_cluster==cls].MyID.values\n",
    "        idx = list(set(idx))\n",
    "        tmp = ret[idx,:]\n",
    "        plt.scatter(tmp[:,0], tmp[:,1], label=cls, alpha=0.6)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=15)\n",
    "    plt.title(\"TSNE\")\n",
    "    if figname!=None:\n",
    "        if not os.path.isdir('local_TSNE'):\n",
    "            os.mkdir('local_TSNE')\n",
    "        fig.savefig(os.path.join('local_TSNE', figname), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_modelweights(model):\n",
    "    nn_weight, gnn_weight = {}, {}\n",
    "    for x in model.encoder.state_dict().keys():\n",
    "        key = x.split('.')\n",
    "        if key[0]=='net_encoder' and key[-1]=='weight':\n",
    "            network = key[1]\n",
    "            route = key[2]\n",
    "            if route=='fc':\n",
    "                nn_weight[network] = torch.pow(model.encoder.state_dict()[x], 2).mean().item()\n",
    "            else:\n",
    "                gnn_weight[network] = torch.pow(model.encoder.state_dict()[x], 2).mean().item()\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    tmp = [list(nn_weight.values()), list(gnn_weight.values())]\n",
    "    sns.heatmap(np.array(tmp), cmap='Blues', yticklabels=['NN', 'GNN'], xticklabels=list(nn_weight.keys()))\n",
    "    plt.xlabel(\"Networks\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUPR(prob, y):\n",
    "    precision, recall, thresh = precision_recall_curve(y, prob)\n",
    "    auprc = auc(recall, precision)\n",
    "    return auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUROC(prob, y):\n",
    "    fpr, tpr, thresh = roc_curve(y, prob)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC(prob, y):\n",
    "    pred = (prob.flatten() >= 0.5).astype(int)\n",
    "    acc = accuracy_score(pred, y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(prob, y):\n",
    "    auprc = AUPR(prob, y)\n",
    "    auroc = AUROC(prob, y)\n",
    "    acc = ACC(prob, y)\n",
    "    ret = {\n",
    "        'AUROC' : auroc,\n",
    "        'AUPRC' : auprc,\n",
    "        'ACC' : acc\n",
    "    }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(pred, target):\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    target = target.detach().cpu().numpy()\n",
    "    metrics = calc_metrics(pred, target)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cos_sim(v1,v2):\n",
    "    return np.dot(v1,v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_max_values(auroc, auprc, acc):\n",
    "    print(\"AUROC: {}\".format(max(auroc)))\n",
    "    print(\"AUPRC: {}\".format(max(auprc)))\n",
    "    print(\"ACC: {}\".format(max(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average_and_std(value):\n",
    "    ret = np.array([max(v) for v in value])\n",
    "    ave = np.mean(ret)\n",
    "    std = np.std(ret)\n",
    "    return ave, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeNorm(nn.Module):\n",
    "    def __init__(self, unbiased=False, eps=1e-6):\n",
    "        super(NodeNorm, self).__init__()\n",
    "        self.unbiased = unbiased\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        std = (torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps).sqrt()\n",
    "        x = (x - mean) / std\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropEdge(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(DropEdge, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def dropedge(self, x, y):\n",
    "        void_dt = np.dtype((np.void, x.dtype.itemsize * np.prod(x.shape[1:])))\n",
    "        orig_dt = np.dtype((x.dtype, x.shape[1:]))\n",
    "\n",
    "        x = np.ascontiguousarray(x)\n",
    "        y = np.ascontiguousarray(y)\n",
    "        x_void = x.reshape(x.shape[0], -1).view(void_dt)\n",
    "        y_void = y.reshape(y.shape[0], -1).view(void_dt)\n",
    "\n",
    "        # Get indices in a that are also in b\n",
    "        return np.setdiff1d(x_void, y_void).view(orig_dt).T\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        if self.dropout == 0:\n",
    "            pass\n",
    "        else:\n",
    "            while True:\n",
    "                rd = np.random.binomial(1, self.dropout, edge_index.shape[1])\n",
    "                mask = rd.nonzero()[0]\n",
    "                if mask.shape[0]!=0:\n",
    "                    break\n",
    "            drop_edge = edge_index[:, mask]\n",
    "            rev_drop_edge = drop_edge.clone()\n",
    "            rev_drop_edge[0, :], rev_drop_edge[1, :] = drop_edge[1, :], drop_edge[0, :]\n",
    "            cat_drop_edge = torch.cat([drop_edge, rev_drop_edge], dim=1)\n",
    "            X = edge_index.T.detach().cpu().numpy()\n",
    "            Y = cat_drop_edge.T.detach().cpu().numpy()\n",
    "            edge_index = Tensor(self.dropedge(X,Y)).long()\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MyGAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, num_nodes, dropout_ratio):\n",
    "        super(MonoEncoder, self).__init__()\n",
    "        self.out_dim = in_dim\n",
    "        self.fc =  nn.Sequential(nn.Linear(in_dim, in_dim),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Dropout(p=dropout_ratio))\n",
    "        #self.conv = MyMessagePassing(in_dim, self.out_dim, num_nodes)\n",
    "        self.conv = GCNConv(in_dim, self.out_dim)\n",
    "        self.node_norm = NodeNorm()\n",
    "    \n",
    "    def forward(self, x, edge_index, route_type, edge_weight=None):\n",
    "        # learn model by selected route\n",
    "        if route_type == \"GNN\":\n",
    "            x = self.conv(x, edge_index, edge_weight=edge_weight)\n",
    "        elif route_type == \"NN\":\n",
    "            x = self.fc(x)\n",
    "        elif route_type == 'SKIP':\n",
    "            x_s = torch.clone(x)\n",
    "            x = self.conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x = x + x_s\n",
    "        elif route_type == \"MIX\":\n",
    "            x_s = torch.clone(x);\n",
    "            x = self.conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x_s = self.fc(x_s);\n",
    "            x = x + x_s\n",
    "        else:\n",
    "            print(\"Route ERROR\");\n",
    "\n",
    "        x = self.node_norm(x)\n",
    "        x = F.leaky_relu(x, inplace=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteEncoder(nn.Module):\n",
    "    def __init__(self, device, in_dim, n_src, n_tar, dropout_ratio):\n",
    "        super(BipartiteEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.out_dim = in_dim\n",
    "        self.n_src = n_src\n",
    "        self.n_tar = n_tar\n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_ratio)\n",
    "        )\n",
    "        #self.conv = MyMessagePassing(in_dim, self.out_dim, num_nodes)\n",
    "        self.conv = GCNConv(in_dim, self.out_dim)\n",
    "        self.node_norm = NodeNorm()\n",
    "        \n",
    "    def forward(self, x_src, x_tar, edge_index, route_type, edge_weight=None):\n",
    "        # concat node embeddings\n",
    "        x = torch.cat([x_src, x_tar], axis=0).to(self.device)\n",
    "        \n",
    "        # learn model by selected route\n",
    "        if route_type == \"GNN\":\n",
    "            x = self.conv(x, edge_index, edge_weight=edge_weight)\n",
    "        elif route_type == \"NN\":\n",
    "            x = self.fc(x)\n",
    "        elif route_type == \"SKIP\":\n",
    "            x_s = torch.clone(x)\n",
    "            x = self.conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x = x + x_s\n",
    "        elif route_type == \"MIX\":\n",
    "            x_s = torch.clone(x);\n",
    "            x = self.conv(x, edge_index, edge_weight=edge_weight)\n",
    "            x_s = self.fc(x_s);\n",
    "            x = x + x_s\n",
    "        else:\n",
    "            print(\"Route ERROR\")\n",
    "            \n",
    "        x = self.node_norm(x)\n",
    "        x = F.leaky_relu(x, inplace=True)\n",
    "        x_src = x[:self.n_src, :]\n",
    "        x_tar = x[self.n_src:, :]\n",
    "        return x_src, x_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, node_num, mask_index, conf):\n",
    "        super(AttributeEncoder, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.node_num = node_num\n",
    "        self.device = conf['device']\n",
    "        self.dropout_ratio = conf['dropout_ratio']\n",
    "        self.mask_index = mask_index\n",
    "        \n",
    "        if mask_index is not None:\n",
    "            self.mask = self.make_mask(self.node_num, self.in_dim, self.mask_index).to(self.device)\n",
    "        else:\n",
    "            self.mask = None\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.in_dim//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(self.in_dim//2, self.in_dim//4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(self.in_dim//4, self.out_dim)\n",
    "        )\n",
    "        \n",
    "    def make_mask(self, num, feat_dim, mask_index):\n",
    "        mask = torch.ones([num, feat_dim])\n",
    "        mask[[mask_index], :] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training and (self.mask is not None):\n",
    "            x = x*self.mask\n",
    "        else:\n",
    "            pass\n",
    "        x = self.fc1(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        x = self.fc3(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECFPEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, node_num, mask_index, conf):\n",
    "        super(ECFPEncoder, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.node_num = node_num\n",
    "        self.device = conf['device']\n",
    "        self.dropout_ratio = conf['dropout_ratio']\n",
    "        self.mask_index = mask_index\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.in_dim//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(self.in_dim//2, self.in_dim//4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(self.in_dim//4, self.out_dim)\n",
    "        )\n",
    "        \n",
    "    def make_mask(self, num, feat_dim, mask_index):\n",
    "        mask = torch.ones([num, feat_dim])\n",
    "        mask[[mask_index], :] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        x = self.fc3(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        # mask for external node's embeddings\n",
    "        if self.training:\n",
    "            x[self.mask_index] = 0\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AminoSeqEncoder(nn.Module):\n",
    "    def __init__(self, uniq_chars, seq_mxlen, node_num, out_dim, channel1, channel2, mask_index, conf, kernel_size=3, stride=2):\n",
    "        super(AminoSeqEncoder, self).__init__()\n",
    "        self.uniq_chars = uniq_chars\n",
    "        self.seq_mxlen = seq_mxlen\n",
    "        self.node_num = node_num\n",
    "        self.out_dim = out_dim\n",
    "        self.channel1 = channel1\n",
    "        self.channel2 = channel2\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = self.kernel_size//2\n",
    "        self.stride = stride\n",
    "        self.dropout = conf['dropout_ratio']\n",
    "        self.device = conf['device']\n",
    "        self.mask_index = mask_index\n",
    "        \n",
    "        if mask_index is not None:\n",
    "            self.mask = self.make_mask(self.mask_index).to(self.device)\n",
    "            self.mask.requires_grad = False\n",
    "        else:\n",
    "            self.mask = None\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                self.uniq_chars, self.channel1, self.kernel_size, self.padding\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.channel1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.stride)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                self.channel1, self.channel2, self.kernel_size, self.padding\n",
    "            ),\n",
    "            nn.BatchNorm1d(self.channel2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.stride)\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(4335, self.out_dim*4),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.out_dim*4, self.out_dim*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.out_dim*2, self.out_dim)\n",
    "        )\n",
    "    \n",
    "    def make_mask(self, mask_index):\n",
    "        mask = torch.ones([self.node_num, self.uniq_chars, self.out_dim])\n",
    "        mask[[mask_index], :] = 0\n",
    "        return mask\n",
    "    \n",
    "    def calc_unique_char(self, seqs):\n",
    "        uniq_chars = set()\n",
    "        char_to_id = {}\n",
    "        for seq in seqs:\n",
    "            uniq_chars = uniq_chars.union(set(list(seq)))\n",
    "        for e,x in enumerate(list(uniq_chars)):\n",
    "            char_to_id[x] = e\n",
    "        return char_to_id\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        # mask for external node's embeddings\n",
    "        if self.training:\n",
    "            x[self.mask_index] = 0\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, data, nsymbols, esymbols, weighted_networks, attributes, conf):\n",
    "        super(MyEncoder, self).__init__()\n",
    "        \n",
    "        # conf\n",
    "        self.device = conf['device']\n",
    "        self.route = conf['enc_type']\n",
    "        self.cycle_num = conf['cycle_num']\n",
    "        self.dropout_ratio = conf['dropout_ratio']\n",
    "        self.network_order = conf['network_order_symbol']\n",
    "        self.target_network = conf['target_network']\n",
    "        \n",
    "        # node counts\n",
    "        self.n_nodes = { key:data['n_{}'.format(key)] for key in nsymbols.keys() }\n",
    "        self.node_symbols = nsymbols\n",
    "        \n",
    "        # edge index\n",
    "        self.edge_index = { network:data['{}_edge_index'.format(network)] for network in esymbols.values() }\n",
    "        self.edge_weight = { esymbols[network]:data['{}_edge_weight'.format(esymbols[network])] for network in weighted_networks }\n",
    "        self.edge_symbols = esymbols\n",
    "        self.weighted_networks = set([esymbols[network] for network in weighted_networks])\n",
    "        \n",
    "        # external drug index\n",
    "        self.src_mask = data.external_src_index\n",
    "        self.tar_mask = data.external_tar_index\n",
    "        \n",
    "        # Model A (network)\n",
    "        self.net_encoder = nn.ModuleDict()\n",
    "        for network, symbol in esymbols.items():\n",
    "            src, tar, *diff = network.split('_')\n",
    "            if src == tar:\n",
    "                self.net_encoder[symbol] = MonoEncoder(\n",
    "                    conf['emb_dim'], data['n_{}'.format(src)], self.dropout_ratio\n",
    "                )\n",
    "            else:\n",
    "                if network == self.target_network:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.net_encoder[symbol] = BipartiteEncoder(\n",
    "                        self.device, conf['emb_dim'], data['n_{}'.format(src)], data['n_{}'.format(tar)], self.dropout_ratio\n",
    "                    )\n",
    "                    \n",
    "        # Model B (Attribute)\n",
    "        ### Drug\n",
    "        self.att_encoder = nn.ModuleDict()\n",
    "        self.att_encoder['drug'] = ECFPEncoder(\n",
    "            attributes['drug_ecfp'], conf['emb_dim'], data['n_drug'], self.src_mask, conf\n",
    "        )\n",
    "        \n",
    "        ### Protein\n",
    "        self.att_encoder['protein'] = AminoSeqEncoder(\n",
    "            attributes['protein_uniqchar'], attributes['protein_mxlen'], data['n_protein'],\n",
    "            conf['emb_dim'], 5, 5, self.tar_mask, conf\n",
    "        )\n",
    "        \n",
    "        # Other modules\n",
    "        self.drop_edge = DropEdge(conf['dropedge_ratio'])\n",
    "        \n",
    "    def forward(self, feat):\n",
    "        \n",
    "        # Model B (attribute)\n",
    "        d_feat_att = self.att_encoder['drug'](feat['d_feat'])\n",
    "        p_feat_att = self.att_encoder['protein'](feat['p_feat'])\n",
    "        feat['d_feat'] = d_feat_att.clone()\n",
    "        feat['p_feat'] = p_feat_att.clone()\n",
    "        \n",
    "        # Model A (network)\n",
    "        for _ in range(self.cycle_num):\n",
    "            for network in self.network_order:\n",
    "                src, tar, *diff = network.split('_')\n",
    "                \n",
    "                if src == tar:\n",
    "                    if network not in self.weighted_networks:\n",
    "                        edge_index = self.drop_edge(self.edge_index[network]).to(self.device)\n",
    "                        feat['{}_feat'.format(src)] = self.net_encoder[network](\n",
    "                            feat['{}_feat'.format(src)], edge_index, self.route\n",
    "                        )\n",
    "                    else:\n",
    "                        edge_index = self.edge_index[network].to(self.device)\n",
    "                        feat['{}_feat'.format(src)] = self.net_encoder[network](\n",
    "                            feat['{}_feat'.format(src)], edge_index, self.route, self.edge_weight[network]\n",
    "                        )\n",
    "                \n",
    "                else:\n",
    "                    if network not in self.weighted_networks:\n",
    "                        edge_index = self.drop_edge(self.edge_index[network]).to(self.device)\n",
    "                        feat['{}_feat'.format(src)], feat['{}_feat'.format(tar)] = self.net_encoder[network](\n",
    "                            feat['{}_feat'.format(src)], feat['{}_feat'.format(tar)],\n",
    "                            edge_index, self.route\n",
    "                        )\n",
    "                    else:\n",
    "                        edge_index = self.edge_index[network].to(self.device)\n",
    "                        feat['{}_feat'.format(src)], feat['{}_feat'.format(tar)] = self.net_encoder[network](\n",
    "                            feat['{}_feat'.format(src)], feat['{}_feat'.format(tar)],\n",
    "                            edge_index, self.route, self.edge_weight[network]\n",
    "                        )\n",
    "\n",
    "        # return final embeddings\n",
    "        ret_feat = { key:feat[key] for key in feat.keys }\n",
    "        ret_feat['d_feat_att'] = d_feat_att\n",
    "        ret_feat['p_feat_att'] = p_feat_att\n",
    "        ret_feat = Data.from_dict(ret_feat)\n",
    "        \n",
    "        return ret_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder_AttributeOnly(nn.Module):\n",
    "    def __init__(self, data, nsymbols, esymbols, weighted_networks, attributes, conf):\n",
    "        super(MyEncoder_AttributeOnly, self).__init__()\n",
    "        \n",
    "        # conf\n",
    "        self.device = conf['device']\n",
    "        self.route = conf['enc_type']\n",
    "        self.cycle_num = conf['cycle_num']\n",
    "        self.dropout_ratio = conf['dropout_ratio']\n",
    "        self.network_order = conf['network_order_symbol']\n",
    "        self.target_network = conf['target_network']\n",
    "        \n",
    "        # node counts\n",
    "        self.n_nodes = { key:data['n_{}'.format(key)] for key in nsymbols.keys() }\n",
    "        self.node_symbols = nsymbols\n",
    "        \n",
    "        # edge index\n",
    "        self.edge_index = { network:data['{}_edge_index'.format(network)] for network in esymbols.values() }\n",
    "        self.edge_weight = { esymbols[network]:data['{}_edge_weight'.format(esymbols[network])] for network in weighted_networks }\n",
    "        self.edge_symbols = esymbols\n",
    "        self.weighted_networks = set([esymbols[network] for network in weighted_networks])\n",
    "        \n",
    "        # external drug index\n",
    "        self.src_mask = data.external_src_index\n",
    "        self.tar_mask = data.external_tar_index\n",
    "        \n",
    "        # Model B (Attribute)\n",
    "        ### Drug\n",
    "        self.att_encoder = nn.ModuleDict()\n",
    "        self.att_encoder['drug'] = ECFPEncoder(\n",
    "            attributes['drug_ecfp'], conf['emb_dim'], data['n_drug'], self.src_mask, conf\n",
    "        )\n",
    "        \n",
    "        ### Protein\n",
    "        self.att_encoder['protein'] = AminoSeqEncoder(\n",
    "            attributes['protein_uniqchar'], attributes['protein_mxlen'], data['n_protein'],\n",
    "            conf['emb_dim'], 5, 5, self.tar_mask, conf\n",
    "        )\n",
    "        \n",
    "        # Other modules\n",
    "        self.drop_edge = DropEdge(conf['dropedge_ratio'])\n",
    "        \n",
    "    def forward(self, feat):\n",
    "        \n",
    "        # Model B (attribute)\n",
    "        d_feat_att = self.att_encoder['drug'](feat['d_feat'])\n",
    "        p_feat_att = self.att_encoder['protein'](feat['p_feat'])\n",
    "        feat['d_feat'] = d_feat_att.clone()\n",
    "        feat['p_feat'] = p_feat_att.clone()\n",
    "\n",
    "        # return final embeddings\n",
    "        ret_feat = { key:feat[key] for key in feat.keys }\n",
    "        ret_feat['d_feat_att'] = d_feat_att\n",
    "        ret_feat['p_feat_att'] = p_feat_att\n",
    "        ret_feat = Data.from_dict(ret_feat)\n",
    "        \n",
    "        return ret_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder_cat(nn.Module):\n",
    "    def __init__(self, in_dim, dropout_ratio):\n",
    "        super(MLPDecoder_cat, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(int(in_dim*2), int(in_dim)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_ratio)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(int(in_dim), int(in_dim//2)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_ratio)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(int(in_dim//2), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_tar, edge_index):\n",
    "        z = torch.cat([z_src[edge_index[0]], z_tar[edge_index[1]]], dim=1)\n",
    "        z = self.layer1(z)\n",
    "        z = self.layer2(z)\n",
    "        z = self.layer3(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder_mul(nn.Module):\n",
    "    def __init__(self, in_dim, dropout_ratio):\n",
    "        super(MLPDecoder_mul, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(int(in_dim), int(in_dim//2)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_ratio)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(int(in_dim//2), int(in_dim//4)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_ratio)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(int(in_dim//4), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_tar, edge_index):\n",
    "        z = torch.mul(z_src[edge_index[0]], z_tar[edge_index[1]])\n",
    "        z = self.layer1(z)\n",
    "        z = self.layer2(z)\n",
    "        z = self.layer3(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "    \n",
    "    def forward(self, z_src, z_tar, edge_index):\n",
    "        z = (z_src[edge_index[0]]*z_tar[edge_index[1]]).sum(dim=1)\n",
    "        z = torch.sigmoid(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(MyDecoder, self).__init__()\n",
    "        self.device = conf['device']\n",
    "        \n",
    "        if conf['dec_type'] == 'CAT':\n",
    "            self.decoder = MLPDecoder_cat(conf['emb_dim'], conf['dropout_ratio'])\n",
    "        elif conf['dec_type'] == 'MIX':\n",
    "            self.decoder = MLPDecoder_mul(conf['emb_dim'], conf['dropout_ratio'])\n",
    "        elif conf['dec_type'] == 'IPD':\n",
    "            self.decoder = InnerProductDecoder()\n",
    "        else:\n",
    "            raise Exception(\"Decoder type error\")\n",
    "    \n",
    "    def forward(self, z_src, z_tar, edge_index):\n",
    "        ret = self.decoder(z_src, z_tar, edge_index)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Valid, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(feat, pos_edge, nsymbol, used, int_src_index, int_tar_index, ext_src_index, ext_tar_index, conf):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    cosemb_loss = CosineEmbeddingLoss()\n",
    "    loss = 0\n",
    "    \n",
    "    ret_feat = model.encoder(feat)\n",
    "    src_nodetype, tar_nodetype = conf['target_network'].split('_')\n",
    "    src_nodesymbol, tar_nodesymbol = nsymbol[src_nodetype], nsymbol[tar_nodetype]\n",
    "    pos_edge_index = pos_edge.clone()\n",
    "    neg_edge_index = negative_sampling_edge_index(\n",
    "        pos_edge_index, data['n_{}'.format(src_nodetype)], data['n_{}'.format(tar_nodetype)],\n",
    "        used, int_src_index, int_tar_index, conf\n",
    "    )\n",
    "    neg_edge_index = neg_edge_index.to(conf['device'])\n",
    "    \n",
    "    pos_score = model.decoder(ret_feat['{}_feat'.format(src_nodesymbol)], ret_feat['{}_feat'.format(tar_nodesymbol)], pos_edge_index)\n",
    "    neg_score = model.decoder(ret_feat['{}_feat'.format(src_nodesymbol)], ret_feat['{}_feat'.format(tar_nodesymbol)], neg_edge_index)\n",
    "    \n",
    "    pos_target = torch.ones(pos_score.shape[0])\n",
    "    neg_target = torch.zeros(neg_score.shape[0])\n",
    "    \n",
    "    score = torch.cat([pos_score, neg_score])\n",
    "    target = torch.cat([pos_target, neg_target])\n",
    "    \n",
    "    # link prediction loss\n",
    "    cls_loss = -torch.log(pos_score+conf['eps']).mean() - torch.log(1-neg_score+conf['eps']).mean()\n",
    "    loss += cls_loss\n",
    "    \n",
    "    \n",
    "    # l1 regularization\n",
    "    if conf['norm_lambda'] != 0:\n",
    "        if conf['enc_type']=='MIX' or conf['enc_type']=='NN':\n",
    "            reg_nn_loss = torch.sum(Tensor([\n",
    "                torch.norm(model.encoder.net_encoder[key].fc[0].weight, conf['reg_type'])*conf['norm_lambda'] \\\n",
    "                    for key in model.encoder.net_encoder.keys()\n",
    "            ]))\n",
    "            loss += reg_nn_loss\n",
    "        if conf['enc_type']=='MIX' or conf['enc_type']=='GNN' or conf['enc_type']=='SKIP':\n",
    "            reg_gnn_loss = torch.sum(Tensor([\n",
    "                torch.norm(model.encoder.net_encoder[key].conv.weight, conf['reg_type'])*conf['norm_lambda'] \\\n",
    "                    for key in model.encoder.net_encoder.keys()\n",
    "            ]))\n",
    "            loss += reg_gnn_loss\n",
    "            \n",
    "    # embedding loss\n",
    "    if conf['emb_loss'] == 'cos':\n",
    "        if conf['task_type'] == 'semi_inductive':\n",
    "            label_src = torch.ones(len(int_src_index)).to(conf['device'])\n",
    "            emb_loss_src = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(src_nodesymbol)][int_src_index],\n",
    "                ret_feat['{}_feat_att'.format(src_nodesymbol)][int_src_index], label_src\n",
    "            )\n",
    "            loss += emb_loss_src\n",
    "        elif conf['task_type'] == 'full_inductive':\n",
    "            label_src = torch.ones(len(int_src_index)).to(conf['device'])\n",
    "            label_tar = torch.ones(len(int_tar_index)).to(conf['device'])\n",
    "            emb_loss_src = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(src_nodesymbol)][int_src_index],\n",
    "                ret_feat['{}_feat_att'.format(src_nodesymbol)][int_src_index], label_src\n",
    "            )\n",
    "            emb_loss_tar = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(tar_nodesymbol)][int_tar_index],\n",
    "                ret_feat['{}_feat_att'.format(tar_nodesymbol)][int_tar_index], label_tar\n",
    "            )\n",
    "            loss += emb_loss_src + emb_loss_tar\n",
    "    elif conf['emb_loss'] == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"Embedding loss function is not defined\")\n",
    "    \n",
    "    # update model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    metrics = evaluation(score, target)\n",
    "    loss = loss.item()\n",
    "    \n",
    "    return ret_feat, loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(feat, valid_edge_index, valid_neg_edge_index, nsymbol, embsymbol, int_src_index, int_tar_index, conf):\n",
    "    model.eval()\n",
    "    cosemb_loss = CosineEmbeddingLoss()\n",
    "    src_nodetype, tar_nodetype = conf['target_network'].split('_')\n",
    "    src_nodesymbol, tar_nodesymbol = nsymbol[src_nodetype], nsymbol[tar_nodetype]\n",
    "    \n",
    "    if conf['task_type'] == 'transductive':\n",
    "        z_src = ret_feat['{}_feat'.format(src_nodesymbol)]\n",
    "        z_tar = ret_feat['{}_feat'.format(tar_nodesymbol)]\n",
    "    elif conf['task_type'] == 'semi_inductive':\n",
    "        z_src = model.encoder.att_encoder[src_nodetype](data[embsymbol[src_nodetype]]).to(conf['device'])\n",
    "        z_tar = ret_feat['{}_feat'.format(tar_nodesymbol)]\n",
    "    elif conf['task_type'] == 'full_inductive':\n",
    "        z_src = model.encoder.att_encoder[src_nodetype](data[embsymbol[src_nodetype]]).to(conf['device'])\n",
    "        z_tar = model.encoder.att_encoder[tar_nodetype](data[embsymbol[tar_nodetype]]).to(conf['device'])\n",
    "    else:\n",
    "        raise Exception(\"Task type error\")\n",
    "    \n",
    "    pos_score = model.decoder(z_src, z_tar, valid_edge_index)\n",
    "    neg_score = model.decoder(z_src, z_tar, valid_neg_edge_index)\n",
    "    \n",
    "    pos_target = torch.ones(pos_score.shape[0])\n",
    "    neg_target = torch.zeros(neg_score.shape[0])\n",
    "    \n",
    "    score = torch.cat([pos_score, neg_score])\n",
    "    target = torch.cat([pos_target, neg_target])\n",
    "    \n",
    "    # link prediction loss\n",
    "    loss = -torch.log(pos_score+conf['eps']).mean() - torch.log(1-neg_score+conf['eps']).mean()\n",
    "    \n",
    "    # l1 regularization\n",
    "    if conf['norm_lambda'] != 0:\n",
    "        if conf['enc_type']=='MIX' or conf['enc_type']=='NN':\n",
    "            reg_nn_loss = torch.sum(Tensor([\n",
    "                torch.norm(model.encoder.net_encoder[key].fc[0].weight, conf['reg_type'])*conf['norm_lambda'] \\\n",
    "                    for key in model.encoder.net_encoder.keys()\n",
    "            ]))\n",
    "            loss += reg_nn_loss\n",
    "        if conf['enc_type']=='MIX' or conf['enc_type']=='GNN' or conf['enc_type']=='SKIP':\n",
    "            reg_gnn_loss = torch.sum(Tensor([\n",
    "                torch.norm(model.encoder.net_encoder[key].conv.weight, conf['reg_type'])*conf['norm_lambda'] \\\n",
    "                    for key in model.encoder.net_encoder.keys()\n",
    "            ]))\n",
    "            loss += reg_gnn_loss\n",
    "            \n",
    "    # embedding loss\n",
    "    if conf['emb_loss'] == 'cos':\n",
    "        if conf['task_type'] == 'semi_inductive':\n",
    "            label_src = torch.ones(len(int_src_index)).to(conf['device'])\n",
    "            emb_loss_src = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(src_nodesymbol)][int_src_index],\n",
    "                ret_feat['{}_feat_att'.format(src_nodesymbol)][int_src_index], label_src\n",
    "            )\n",
    "            loss += emb_loss_src\n",
    "        elif conf['task_type'] == 'full_inductive':\n",
    "            label_src = torch.ones(len(int_src_index)).to(conf['device'])\n",
    "            label_tar = torch.ones(len(int_tar_index)).to(conf['device'])\n",
    "            emb_loss_src = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(src_nodesymbol)][int_src_index],\n",
    "                ret_feat['{}_feat_att'.format(src_nodesymbol)][int_src_index], label_src\n",
    "            )\n",
    "            emb_loss_tar = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(tar_nodesymbol)][int_tar_index],\n",
    "                ret_feat['{}_feat_att'.format(tar_nodesymbol)][int_tar_index], label_tar\n",
    "            )\n",
    "            loss += emb_loss_src + emb_loss_tar\n",
    "            \n",
    "    elif conf['emb_loss'] == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"Embedding loss function is not defined\")\n",
    "    \n",
    "    metrics = evaluation(score, target)\n",
    "    loss = loss.item()\n",
    "    \n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(feat, test_edge_index, test_neg_edge_index, nsymbol, embsymbol, int_src_index, int_tar_index, conf):\n",
    "    model.eval()\n",
    "    cosemb_loss = CosineEmbeddingLoss()\n",
    "    src_nodetype, tar_nodetype = conf['target_network'].split('_')\n",
    "    src_nodesymbol, tar_nodesymbol = nsymbol[src_nodetype], nsymbol[tar_nodetype]\n",
    "    \n",
    "    if conf['task_type'] == 'transductive':\n",
    "        z_src = ret_feat['{}_feat'.format(src_nodesymbol)]\n",
    "        z_tar = ret_feat['{}_feat'.format(tar_nodesymbol)]\n",
    "    elif conf['task_type'] == 'semi_inductive':\n",
    "        z_src = model.encoder.att_encoder[src_nodetype](data[embsymbol[src_nodetype]]).to(conf['device'])\n",
    "        z_tar = ret_feat['{}_feat'.format(tar_nodesymbol)]\n",
    "    elif conf['task_type'] == 'full_inductive':\n",
    "        z_src = model.encoder.att_encoder[src_nodetype](data[embsymbol[src_nodetype]]).to(conf['device'])\n",
    "        z_tar = model.encoder.att_encoder[tar_nodetype](data[embsymbol[tar_nodetype]]).to(conf['device'])\n",
    "    else:\n",
    "        raise Exception(\"Task type error\")\n",
    "    \n",
    "    pos_score = model.decoder(z_src, z_tar, test_edge_index)\n",
    "    neg_score = model.decoder(z_src, z_tar, test_neg_edge_index)\n",
    "    \n",
    "    pos_target = torch.ones(pos_score.shape[0])\n",
    "    neg_target = torch.zeros(neg_score.shape[0])\n",
    "    \n",
    "    score = torch.cat([pos_score, neg_score])\n",
    "    target = torch.cat([pos_target, neg_target])\n",
    "    \n",
    "    # link prediction loss\n",
    "    loss = -torch.log(pos_score+conf['eps']).mean() - torch.log(1-neg_score+conf['eps']).mean()\n",
    "    \n",
    "    # l1 regularization\n",
    "    if conf['norm_lambda'] != 0:\n",
    "        if conf['enc_type']=='MIX' or conf['enc_type']=='NN':\n",
    "            reg_nn_loss = torch.sum(Tensor([\n",
    "                torch.norm(model.encoder.net_encoder[key].fc[0].weight, conf['reg_type'])*conf['norm_lambda'] \\\n",
    "                    for key in model.encoder.net_encoder.keys()\n",
    "            ]))\n",
    "            loss += reg_nn_loss\n",
    "        if conf['enc_type']=='MIX' or conf['enc_type']=='GNN' or conf['enc_type']=='SKIP':\n",
    "            reg_gnn_loss = torch.sum(Tensor([\n",
    "                torch.norm(model.encoder.net_encoder[key].conv.weight, conf['reg_type'])*conf['norm_lambda'] \\\n",
    "                    for key in model.encoder.net_encoder.keys()\n",
    "            ]))\n",
    "            loss += reg_gnn_loss\n",
    "            \n",
    "    # embedding loss\n",
    "    if conf['emb_loss'] == 'cos':\n",
    "        if conf['task_type'] == 'semi_inductive':\n",
    "            label_src = torch.ones(len(int_src_index)).to(conf['device'])\n",
    "            emb_loss_src = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(src_nodesymbol)][int_src_index],\n",
    "                ret_feat['{}_feat_att'.format(src_nodesymbol)][int_src_index], label_src\n",
    "            )\n",
    "            loss += emb_loss_src\n",
    "        elif conf['task_type'] == 'full_inductive':\n",
    "            label_src = torch.ones(len(int_src_index)).to(conf['device'])\n",
    "            label_tar = torch.ones(len(int_tar_index)).to(conf['device'])\n",
    "            emb_loss_src = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(src_nodesymbol)][int_src_index],\n",
    "                ret_feat['{}_feat_att'.format(src_nodesymbol)][int_src_index], label_src\n",
    "            )\n",
    "            emb_loss_tar = cosemb_loss(\n",
    "                ret_feat['{}_feat'.format(tar_nodesymbol)][int_tar_index],\n",
    "                ret_feat['{}_feat_att'.format(tar_nodesymbol)][int_tar_index], label_tar\n",
    "            )\n",
    "            loss += emb_loss_src + emb_loss_tar\n",
    "    elif conf['emb_loss'] == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"Embedding loss function is not defined\")\n",
    "    \n",
    "    metrics = evaluation(score, target)\n",
    "    loss = loss.item()\n",
    "    \n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datafile & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT PATH: /home/jupyter-user/workspace/MyLab/STRGNN/src\n",
      "DATASET PATH: /home/jupyter-user/workspace/MyLab/STRGNN/Dataset\n"
     ]
    }
   ],
   "source": [
    "cur_path = os.getcwd()\n",
    "dataset_path = os.path.join(get_parent_path(cur_path, 1), \"Dataset\")\n",
    "print(\"CURRENT PATH: {}\".format(cur_path))\n",
    "print(\"DATASET PATH: {}\".format(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes\n",
    "node_names = ['drug', 'disease', 'protein', 'mrna', 'mirna', 'metabolite']\n",
    "node_symbols = {\n",
    "    'drug': 'd',\n",
    "    'disease': 'z',\n",
    "    'protein': 'p',\n",
    "    'mrna': 'm',\n",
    "    'mirna': 'mi',\n",
    "    'metabolite': 'b'\n",
    "}\n",
    "symbol_to_nodename = {v:k for k,v in node_symbols.items()}\n",
    "nodes = load_nodes(dataset_path, node_names)\n",
    "node_nums = {node:count_unique_ids(nodes[node]) for node in node_names}\n",
    "did_to_clst = {myid:clst for myid, clst in zip(nodes['drug'].MyID.values, nodes['drug'].ecfp_cluster.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unweighted edgesd\n",
    "unweighted_edge_names = [\n",
    "    'disease_metabolite',\n",
    "    'disease_mirna',\n",
    "    'disease_mrna_down',\n",
    "    'disease_mrna_up',\n",
    "    'disease_protein',\n",
    "    'drug_disease',\n",
    "    'drug_drug',\n",
    "    'drug_mrna_down',\n",
    "    'drug_mrna_up',\n",
    "    'drug_metabolite_down',\n",
    "    'drug_metabolite_up',\n",
    "    'drug_protein',\n",
    "    'protein_protein',\n",
    "]\n",
    "\n",
    "weighted_edge_names = [\n",
    "    'disease_disease',\n",
    "    'drug_mirna',\n",
    "    'protein_mrna',\n",
    "    'protein_mirna',\n",
    "    'mrna_mrna',\n",
    "    'mrna_mirna',\n",
    "    'mirna_mirna'\n",
    "]\n",
    "\n",
    "edge_symbols = {\n",
    "    'disease_disease': 'z_z',\n",
    "    'disease_metabolite': 'z_b',\n",
    "    'disease_mirna': 'z_mi',\n",
    "    'disease_mrna_down': 'z_m_down',\n",
    "    'disease_mrna_up': 'z_m_up',\n",
    "    'disease_protein': 'z_p',\n",
    "    'drug_disease': 'd_z',\n",
    "    'drug_drug': 'd_d',\n",
    "    'drug_mrna_down': 'd_m_down',\n",
    "    'drug_mrna_up': 'd_m_up',\n",
    "    'drug_metabolite_down': 'd_b_down',\n",
    "    'drug_metabolite_up': 'd_b_up',\n",
    "    'drug_protein': 'd_p',\n",
    "    'drug_mirna': 'd_mi',\n",
    "    'mirna_mirna': 'mi_mi',\n",
    "    'mrna_mirna': 'm_mi',\n",
    "    'mrna_mrna': 'm_m',\n",
    "    'protein_protein': 'p_p',\n",
    "    'protein_mirna': 'p_mi',\n",
    "    'protein_mrna': 'p_m'\n",
    "}\n",
    "\n",
    "symbol_to_edgename = {v:k for k,v in edge_symbols.items()}\n",
    "edges, edge_weights = load_edges(dataset_path, unweighted_edge_names, weighted_edge_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3214/3214 [00:02<00:00, 1442.85it/s]\n",
      "100%|| 14981/14981 [00:09<00:00, 1510.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# attributes information\n",
    "emb_names = [\n",
    "    'drug_ecfp', 'protein_seq'\n",
    "]\n",
    "emb_symbols = {\n",
    "    'drug': 'drug_ecfp',\n",
    "    'protein': 'protein_seq'\n",
    "}\n",
    "embs = load_embs(nodes, emb_names, node_nums, MXLEN=nodes['protein'].SeqLenght.max())\n",
    "emb_dims = {\n",
    "    'drug_ecfp': embs['drug_ecfp'].shape[1],\n",
    "    'protein_uniqchar': embs['protein_seq'].shape[1],\n",
    "    'protein_mxlen': embs['protein_seq'].shape[2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    # configuration: model definition parameters\n",
    "    'target_network': 'drug_disease',\n",
    "    'task_type': 'transductive', # 'transductive' or 'semi_inductive' or 'full_inductive'\n",
    "    'emb_loss': 'none', # 'cos' or 'none'\n",
    "    'enc_type': 'MIX', # \"MIX\" or \"GNN\" or \"NN\" or \"SKIP\"\n",
    "    'dec_type': 'IPD', # \"CAT\" or \"MUL\" or \"IPD\"\n",
    "    'negative_style': 'simple', # \"simple\" or \"hard\"\n",
    "\n",
    "    # configuration: training confition parameters\n",
    "    'reg_type': 1,\n",
    "    'lr': 0.001,\n",
    "    'emb_dim': 128,\n",
    "    'cycle_num': 2,\n",
    "    'norm_lambda': 1e-8,\n",
    "    'dropedge_ratio': 0.2,\n",
    "    'dropout_ratio': 0.5,\n",
    "\n",
    "    # configuration: other parameters\n",
    "    'cuda': 2,\n",
    "    'cv': 1,\n",
    "    'eps': 1e-6,\n",
    "    'epoch_num': 1000,\n",
    "    'verbose': 10,\n",
    "}\n",
    "conf['source_node'], conf['target_node'] = conf['target_network'].split('_')\n",
    "\n",
    "# task type check\n",
    "if conf['target_network'] == 'drug_disease':\n",
    "    assert conf['task_type']=='transductive' or conf['task_type']=='semi_inductive'\n",
    "elif conf['target_network'] == 'drug_protein':\n",
    "    assert conf['task_type']=='transductive' or conf['task_type']=='full_inductive' or conf['task_type']=='semi_inductive'\n",
    "else:\n",
    "    raise Exception(\"Configuration error\")\n",
    "\n",
    "# negative sampling style check\n",
    "if conf['task_type']=='transductive':\n",
    "    assert conf['negative_style']=='simple'\n",
    "elif conf['task_type']=='semi_inductive':\n",
    "    assert conf['negative_style']=='simple' or conf['negative_style']=='hard'\n",
    "elif conf['task_type']=='full_inductive':\n",
    "    assert conf['negative_style']=='hard'\n",
    "else:\n",
    "    raise Exception(\"Configuration error\")\n",
    "    \n",
    "# configuration: device\n",
    "conf['device'] = torch.device(\"cuda:{}\".format(conf['cuda']) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# configuration: network order\n",
    "conf['network_order'] = [\n",
    "    'disease_disease',\n",
    "    'disease_metabolite',\n",
    "    'disease_mirna',\n",
    "    'disease_mrna_up',\n",
    "    'disease_mrna_down',\n",
    "    'disease_protein',\n",
    "    'protein_protein',\n",
    "    'protein_mirna',\n",
    "    'protein_mrna',\n",
    "    'mrna_mrna',\n",
    "    'mrna_mirna',\n",
    "    'mirna_mirna',\n",
    "    'drug_mirna',\n",
    "    'drug_mrna_up',\n",
    "    'drug_mrna_down',\n",
    "    'drug_metabolite_up',\n",
    "    'drug_metabolite_down',\n",
    "    'drug_protein',\n",
    "    'drug_drug'\n",
    "]\n",
    "\n",
    "assert conf['target_network'] not in conf['network_order']\n",
    "conf['network_order_symbol'] = [edge_symbols[network] for network in conf['network_order']]\n",
    "\n",
    "# save model\n",
    "conf['save_model'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data.from_dict(dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load node counts\n",
    "for node in node_symbols.keys():\n",
    "    data['n_{}'.format(node)] = node_nums[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edge indexes\n",
    "for network in edge_symbols.keys():\n",
    "    data['{}_edge_index'.format(edge_symbols[network])] = edges[network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load edge weights\n",
    "for network in weighted_edge_names:\n",
    "    data['{}_edge_weight'.format(edge_symbols[network])] = edge_weights[network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial feature\n",
    "for node in node_names:\n",
    "    data['{}_feat'.format(node_symbols[node])] = get_initial_feat(data['n_{}'.format(node)], conf['emb_dim'])\n",
    "\n",
    "# attributes for emb-nodes\n",
    "for key in emb_names:\n",
    "    data[key] = Tensor(embs[key])\n",
    "    data[key].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transductive link prediction\n"
     ]
    }
   ],
   "source": [
    "# data split\n",
    "data = split_link_prediction_datas(data, nodes, edge_symbols, weighted_edge_names, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling for valid / test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Positive samples\" add to used pair\n",
    "raw_used = get_pair(data['{}_edge_index'.format(edge_symbols[conf['target_network']])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.valid_neg_edge_index = []\n",
    "src_node, tar_node = conf['target_network'].split('_')\n",
    "if conf['task_type']=='transductive':\n",
    "    current_src_index = data.internal_src_index\n",
    "    current_tar_index = data.internal_tar_index\n",
    "\n",
    "if conf['task_type']=='semi_inductive':\n",
    "    current_src_index = data.external_src_index\n",
    "    current_tar_index = data.internal_tar_index\n",
    "\n",
    "if conf['task_type']=='full_inductive':\n",
    "    current_src_index = data.external_src_index\n",
    "    current_tar_index = data.external_tar_index\n",
    "\n",
    "# Negative sampling for validation edges\n",
    "for i in range(conf['cv']):\n",
    "    valid_neg_edge_index = negative_sampling_edge_index(\n",
    "        data.valid_edge_index[i], data['n_{}'.format(src_node)], data['n_{}'.format(tar_node)],\n",
    "        raw_used, current_src_index, current_tar_index, conf\n",
    "    )\n",
    "    data.valid_neg_edge_index.append(valid_neg_edge_index)\n",
    "\n",
    "# Negative sampling for test edges\n",
    "data.test_neg_edge_index = negative_sampling_edge_index(\n",
    "    data.test_edge_index, data['n_{}'.format(src_node)], data['n_{}'.format(tar_node)],\n",
    "    raw_used, current_src_index, current_tar_index, conf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Negative samples for valid/test data\" add to used pair\n",
    "used = []\n",
    "for i in range(conf['cv']):\n",
    "    cv_used = copy.deepcopy(raw_used)\n",
    "    neg_pair = get_pair(data.valid_neg_edge_index[i])\n",
    "    cv_used = union_set(cv_used, neg_pair)\n",
    "\n",
    "    neg_pair = get_pair(data.test_neg_edge_index)\n",
    "    cv_used = union_set(cv_used, neg_pair)\n",
    "    used.append(cv_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Task  : transductive\n",
      "Target: drug_disease\n",
      "########################################\n",
      "Node counts >>\n",
      "\tdrug: 3214\n",
      "\tdisease: 1870\n",
      "\tprotein: 14981\n",
      "\tmrna: 20900\n",
      "\tmirna: 3643\n",
      "\tmetabolite: 2156\n",
      "\n",
      "Edge counts >>\n",
      "\tdisease_metabolite: 1672\n",
      "\tdisease_mirna: 2257\n",
      "\tdisease_mrna_down: 14075\n",
      "\tdisease_mrna_up: 14141\n",
      "\tdisease_protein: 5530\n",
      "\tdrug_disease: 5820\n",
      "\tdrug_drug: 1498356\n",
      "\tdrug_mrna_down: 19837\n",
      "\tdrug_mrna_up: 22827\n",
      "\tdrug_metabolite_down: 297\n",
      "\tdrug_metabolite_up: 246\n",
      "\tdrug_protein: 3781\n",
      "\tprotein_protein: 313481\n",
      "\tdisease_disease: 539651\n",
      "\tdrug_mirna: 1779\n",
      "\tprotein_mrna: 75776\n",
      "\tprotein_mirna: 781\n",
      "\tmrna_mrna: 109384\n",
      "\tmrna_mirna: 1495526\n",
      "\tmirna_mirna: 428\n",
      "########################################\n",
      "Node splits >> \n",
      "\tInternal drug nodes: 3214\n",
      "\tInternal disease nodes: 1870\n",
      "\n",
      "\tExternal drug nodes: 0\n",
      "\tExternal disease nodes: 0\n",
      "########################################\n",
      "Edge splits >> \n",
      "\tTrain (cv-0): INT >> 7004\n",
      "\tValid (cv-0): INT >> 1808\n",
      "\n",
      "\tTest: INT >> 3212\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "describe_dataset(data, nodes, edges, edge_symbols, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bipartite graphs\n",
    "for network, symbol in edge_symbols.items():\n",
    "    src, tar, *diff = symbol.split('_')\n",
    "    if network == conf['target_network']:\n",
    "        continue\n",
    "        \n",
    "    if src != tar:\n",
    "        data['{}_edge_index'.format(symbol)] = to_undirected(\n",
    "            data['{}_edge_index'.format(symbol)], data['n_{}'.format(symbol_to_nodename[src], is_bipartite=True)]\n",
    "        )\n",
    "        if network in weighted_edge_names:\n",
    "            data['{}_edge_weight'.format(symbol)] = data['{}_edge_weight'.format(symbol)].repeat(2)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(conf['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training ( link prediction task )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = defaultdict(list), defaultdict(list)\n",
    "train_records, valid_records = defaultdict(list), defaultdict(list)\n",
    "loss_dicts = defaultdict(list)\n",
    "best_records = defaultdict(list)\n",
    "best_states = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--- LINK PREDICTION TYPE: transductive---\n",
      "\n",
      ">>> cross validation: 1 <<<\n",
      "Train >> EPOCH:  1  AUROC:0.5405  AUPRC:0.5887 ACC:0.3333  TIME:3.18\n",
      "Valid >> EPOCH:  1  AUROC:0.5441  AUPRC:0.5939 ACC:0.3333  TIME:3.19\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH: 10  VALID_LOSS:1.3626  AUROC:0.6538  AUPRC:0.4533 ACC:0.5999  TIME:2.76\n",
      "Valid >> EPOCH: 10  VALID_LOSS:1.3620  AUROC:0.6581  AUPRC:0.4603 ACC:0.6018  TIME:2.77\n",
      "Train >> EPOCH: 20  VALID_LOSS:1.2737  AUROC:0.6981  AUPRC:0.5147 ACC:0.6569  TIME:2.76\n",
      "Valid >> EPOCH: 20  VALID_LOSS:1.2685  AUROC:0.7019  AUPRC:0.5226 ACC:0.6514  TIME:2.77\n",
      "Train >> EPOCH: 30  VALID_LOSS:1.2591  AUROC:0.7017  AUPRC:0.5263 ACC:0.6709  TIME:2.74\n",
      "Valid >> EPOCH: 30  VALID_LOSS:1.2550  AUROC:0.7052  AUPRC:0.5293 ACC:0.6624  TIME:2.75\n",
      "Train >> EPOCH: 40  VALID_LOSS:1.2616  AUROC:0.6999  AUPRC:0.5309 ACC:0.6703  TIME:2.75\n",
      "Valid >> EPOCH: 40  VALID_LOSS:1.2569  AUROC:0.7023  AUPRC:0.5359 ACC:0.6692  TIME:2.76\n",
      "Train >> EPOCH: 50  VALID_LOSS:1.2567  AUROC:0.7014  AUPRC:0.5407 ACC:0.6666  TIME:2.74\n",
      "Valid >> EPOCH: 50  VALID_LOSS:1.2483  AUROC:0.7075  AUPRC:0.5524 ACC:0.6622  TIME:2.74\n",
      "Train >> EPOCH: 60  VALID_LOSS:1.2365  AUROC:0.7156  AUPRC:0.5651 ACC:0.6760  TIME:2.75\n",
      "Valid >> EPOCH: 60  VALID_LOSS:1.2357  AUROC:0.7168  AUPRC:0.5678 ACC:0.6652  TIME:2.76\n",
      "Train >> EPOCH: 70  VALID_LOSS:1.2234  AUROC:0.7251  AUPRC:0.5804 ACC:0.6798  TIME:2.74\n",
      "Valid >> EPOCH: 70  VALID_LOSS:1.2310  AUROC:0.7203  AUPRC:0.5731 ACC:0.6737  TIME:2.75\n",
      "Train >> EPOCH: 80  VALID_LOSS:1.2147  AUROC:0.7315  AUPRC:0.5859 ACC:0.6991  TIME:2.72\n",
      "Valid >> EPOCH: 80  VALID_LOSS:1.2099  AUROC:0.7347  AUPRC:0.5896 ACC:0.6963  TIME:2.73\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH: 90  VALID_LOSS:1.2151  AUROC:0.7286  AUPRC:0.5918 ACC:0.6964  TIME:2.73\n",
      "Valid >> EPOCH: 90  VALID_LOSS:1.2095  AUROC:0.7322  AUPRC:0.5934 ACC:0.6914  TIME:2.74\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:100  VALID_LOSS:1.2094  AUROC:0.7318  AUPRC:0.5932 ACC:0.6950  TIME:2.77\n",
      "Valid >> EPOCH:100  VALID_LOSS:1.1971  AUROC:0.7387  AUPRC:0.6091 ACC:0.6932  TIME:2.78\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:110  VALID_LOSS:1.2069  AUROC:0.7315  AUPRC:0.5995 ACC:0.6972  TIME:2.85\n",
      "Valid >> EPOCH:110  VALID_LOSS:1.2008  AUROC:0.7359  AUPRC:0.6030 ACC:0.7028  TIME:2.86\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:120  VALID_LOSS:1.2010  AUROC:0.7371  AUPRC:0.6019 ACC:0.6966  TIME:2.77\n",
      "Valid >> EPOCH:120  VALID_LOSS:1.1949  AUROC:0.7407  AUPRC:0.6068 ACC:0.6971  TIME:2.78\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:130  VALID_LOSS:1.1806  AUROC:0.7508  AUPRC:0.6169 ACC:0.7087  TIME:2.74\n",
      "Valid >> EPOCH:130  VALID_LOSS:1.1840  AUROC:0.7476  AUPRC:0.6183 ACC:0.7063  TIME:2.75\n",
      "Train >> EPOCH:140  VALID_LOSS:1.1888  AUROC:0.7439  AUPRC:0.6098 ACC:0.7074  TIME:2.76\n",
      "Valid >> EPOCH:140  VALID_LOSS:1.1910  AUROC:0.7444  AUPRC:0.6085 ACC:0.6989  TIME:2.77\n",
      "Train >> EPOCH:150  VALID_LOSS:1.1925  AUROC:0.7418  AUPRC:0.6109 ACC:0.7150  TIME:2.83\n",
      "Valid >> EPOCH:150  VALID_LOSS:1.1946  AUROC:0.7379  AUPRC:0.6080 ACC:0.7126  TIME:2.84\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:160  VALID_LOSS:1.1843  AUROC:0.7456  AUPRC:0.6232 ACC:0.7004  TIME:2.74\n",
      "Valid >> EPOCH:160  VALID_LOSS:1.1752  AUROC:0.7507  AUPRC:0.6318 ACC:0.7022  TIME:2.75\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:170  VALID_LOSS:1.1661  AUROC:0.7561  AUPRC:0.6293 ACC:0.7226  TIME:2.72\n",
      "Valid >> EPOCH:170  VALID_LOSS:1.1622  AUROC:0.7589  AUPRC:0.6306 ACC:0.7249  TIME:2.73\n",
      "Train >> EPOCH:180  VALID_LOSS:1.1693  AUROC:0.7548  AUPRC:0.6265 ACC:0.7151  TIME:2.73\n",
      "Valid >> EPOCH:180  VALID_LOSS:1.1697  AUROC:0.7537  AUPRC:0.6255 ACC:0.7133  TIME:2.74\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:190  VALID_LOSS:1.1633  AUROC:0.7577  AUPRC:0.6305 ACC:0.7276  TIME:2.84\n",
      "Valid >> EPOCH:190  VALID_LOSS:1.1693  AUROC:0.7549  AUPRC:0.6229 ACC:0.7222  TIME:2.85\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:200  VALID_LOSS:1.1595  AUROC:0.7607  AUPRC:0.6362 ACC:0.7152  TIME:2.95\n",
      "Valid >> EPOCH:200  VALID_LOSS:1.1686  AUROC:0.7565  AUPRC:0.6249 ACC:0.7089  TIME:2.96\n",
      "Train >> EPOCH:210  VALID_LOSS:1.1561  AUROC:0.7606  AUPRC:0.6365 ACC:0.7361  TIME:2.77\n",
      "Valid >> EPOCH:210  VALID_LOSS:1.1645  AUROC:0.7574  AUPRC:0.6290 ACC:0.7266  TIME:2.78\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:220  VALID_LOSS:1.1542  AUROC:0.7642  AUPRC:0.6424 ACC:0.7211  TIME:2.77\n",
      "Valid >> EPOCH:220  VALID_LOSS:1.1555  AUROC:0.7638  AUPRC:0.6452 ACC:0.7131  TIME:2.78\n",
      "Train >> EPOCH:230  VALID_LOSS:1.1465  AUROC:0.7700  AUPRC:0.6443 ACC:0.7216  TIME:2.94\n",
      "Valid >> EPOCH:230  VALID_LOSS:1.1484  AUROC:0.7693  AUPRC:0.6474 ACC:0.7181  TIME:2.95\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:240  VALID_LOSS:1.1379  AUROC:0.7737  AUPRC:0.6535 ACC:0.7357  TIME:2.74\n",
      "Valid >> EPOCH:240  VALID_LOSS:1.1355  AUROC:0.7741  AUPRC:0.6608 ACC:0.7319  TIME:2.75\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:250  VALID_LOSS:1.1236  AUROC:0.7870  AUPRC:0.6582 ACC:0.7388  TIME:2.75\n",
      "Valid >> EPOCH:250  VALID_LOSS:1.1300  AUROC:0.7847  AUPRC:0.6523 ACC:0.7266  TIME:2.76\n",
      "Train >> EPOCH:260  VALID_LOSS:1.1203  AUROC:0.7896  AUPRC:0.6637 ACC:0.7384  TIME:2.78\n",
      "Valid >> EPOCH:260  VALID_LOSS:1.1285  AUROC:0.7859  AUPRC:0.6489 ACC:0.7345  TIME:2.79\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:270  VALID_LOSS:1.1187  AUROC:0.7941  AUPRC:0.6714 ACC:0.7516  TIME:2.77\n",
      "Valid >> EPOCH:270  VALID_LOSS:1.1262  AUROC:0.7907  AUPRC:0.6507 ACC:0.7478  TIME:2.78\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:280  VALID_LOSS:1.1236  AUROC:0.7873  AUPRC:0.6568 ACC:0.7451  TIME:2.76\n",
      "Valid >> EPOCH:280  VALID_LOSS:1.1241  AUROC:0.7878  AUPRC:0.6561 ACC:0.7365  TIME:2.77\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:290  VALID_LOSS:1.1216  AUROC:0.7919  AUPRC:0.6628 ACC:0.7444  TIME:2.77\n",
      "Valid >> EPOCH:290  VALID_LOSS:1.1169  AUROC:0.7948  AUPRC:0.6613 ACC:0.7463  TIME:2.78\n",
      "Train >> EPOCH:300  VALID_LOSS:1.1058  AUROC:0.8008  AUPRC:0.6657 ACC:0.7412  TIME:2.74\n",
      "Valid >> EPOCH:300  VALID_LOSS:1.1072  AUROC:0.8006  AUPRC:0.6645 ACC:0.7347  TIME:2.75\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:310  VALID_LOSS:1.1017  AUROC:0.8020  AUPRC:0.6713 ACC:0.7386  TIME:2.79\n",
      "Valid >> EPOCH:310  VALID_LOSS:1.1117  AUROC:0.7974  AUPRC:0.6592 ACC:0.7375  TIME:2.80\n",
      "Train >> EPOCH:320  VALID_LOSS:1.1056  AUROC:0.8004  AUPRC:0.6639 ACC:0.7312  TIME:2.77\n",
      "Valid >> EPOCH:320  VALID_LOSS:1.0957  AUROC:0.8052  AUPRC:0.6679 ACC:0.7358  TIME:2.78\n",
      "Train >> EPOCH:330  VALID_LOSS:1.0974  AUROC:0.8004  AUPRC:0.6759 ACC:0.7480  TIME:2.76\n",
      "Valid >> EPOCH:330  VALID_LOSS:1.1007  AUROC:0.7987  AUPRC:0.6763 ACC:0.7424  TIME:2.77\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:340  VALID_LOSS:1.1008  AUROC:0.7985  AUPRC:0.6774 ACC:0.7471  TIME:2.75\n",
      "Valid >> EPOCH:340  VALID_LOSS:1.0993  AUROC:0.8002  AUPRC:0.6752 ACC:0.7461  TIME:2.76\n",
      "Train >> EPOCH:350  VALID_LOSS:1.0817  AUROC:0.8119  AUPRC:0.6806 ACC:0.7414  TIME:2.90\n",
      "Valid >> EPOCH:350  VALID_LOSS:1.0829  AUROC:0.8119  AUPRC:0.6789 ACC:0.7419  TIME:2.91\n",
      "Train >> EPOCH:360  VALID_LOSS:1.0829  AUROC:0.8113  AUPRC:0.6793 ACC:0.7454  TIME:2.79\n",
      "Valid >> EPOCH:360  VALID_LOSS:1.0913  AUROC:0.8079  AUPRC:0.6656 ACC:0.7378  TIME:2.80\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:370  VALID_LOSS:1.0830  AUROC:0.8111  AUPRC:0.6774 ACC:0.7449  TIME:2.80\n",
      "Valid >> EPOCH:370  VALID_LOSS:1.0901  AUROC:0.8068  AUPRC:0.6744 ACC:0.7371  TIME:2.81\n",
      "Train >> EPOCH:380  VALID_LOSS:1.0749  AUROC:0.8160  AUPRC:0.6818 ACC:0.7394  TIME:2.80\n",
      "Valid >> EPOCH:380  VALID_LOSS:1.0858  AUROC:0.8094  AUPRC:0.6680 ACC:0.7310  TIME:2.81\n",
      "Train >> EPOCH:390  VALID_LOSS:1.0725  AUROC:0.8179  AUPRC:0.6808 ACC:0.7394  TIME:2.81\n",
      "Valid >> EPOCH:390  VALID_LOSS:1.0825  AUROC:0.8115  AUPRC:0.6729 ACC:0.7306  TIME:2.82\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:400  VALID_LOSS:1.0712  AUROC:0.8176  AUPRC:0.6840 ACC:0.7387  TIME:2.74\n",
      "Valid >> EPOCH:400  VALID_LOSS:1.0703  AUROC:0.8163  AUPRC:0.6932 ACC:0.7319  TIME:2.75\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:410  VALID_LOSS:1.0695  AUROC:0.8229  AUPRC:0.6932 ACC:0.7627  TIME:2.76\n",
      "Valid >> EPOCH:410  VALID_LOSS:1.0839  AUROC:0.8138  AUPRC:0.6813 ACC:0.7489  TIME:2.77\n",
      "Train >> EPOCH:420  VALID_LOSS:1.0629  AUROC:0.8234  AUPRC:0.6993 ACC:0.7292  TIME:2.84\n",
      "Valid >> EPOCH:420  VALID_LOSS:1.0837  AUROC:0.8136  AUPRC:0.6830 ACC:0.7172  TIME:2.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train >> EPOCH:430  VALID_LOSS:1.0543  AUROC:0.8265  AUPRC:0.6884 ACC:0.7458  TIME:2.90\n",
      "Valid >> EPOCH:430  VALID_LOSS:1.0618  AUROC:0.8206  AUPRC:0.6941 ACC:0.7424  TIME:2.91\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:440  VALID_LOSS:1.0642  AUROC:0.8209  AUPRC:0.6911 ACC:0.7442  TIME:2.75\n",
      "Valid >> EPOCH:440  VALID_LOSS:1.0635  AUROC:0.8199  AUPRC:0.6986 ACC:0.7400  TIME:2.76\n",
      "\t-- UPDATE BEST MODEL --\n",
      "Train >> EPOCH:450  VALID_LOSS:1.0514  AUROC:0.8274  AUPRC:0.6914 ACC:0.7510  TIME:2.76\n",
      "Valid >> EPOCH:450  VALID_LOSS:1.0670  AUROC:0.8195  AUPRC:0.6821 ACC:0.7386  TIME:2.77\n",
      "\t-- UPDATE BEST MODEL --\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t--- LINK PREDICTION TYPE: {}---\".format(conf['task_type']))\n",
    "for cv in range(conf['cv']):\n",
    "    print(\"\\n>>> cross validation: {} <<<\".format(cv+1))\n",
    "    \n",
    "    \"\"\" Define model \"\"\"\n",
    "    epoch_num = 0\n",
    "    best_auprc = 0\n",
    "    encoder = MyEncoder(data, node_symbols, edge_symbols, weighted_edge_names, emb_dims, conf)\n",
    "    decoder = MyDecoder(conf)\n",
    "    model = MyGAE(encoder, decoder).to(conf['device'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=conf['lr'])\n",
    "    \n",
    "    \"\"\" Model learning \"\"\"\n",
    "    for epoch in range(conf['epoch_num'] - epoch_num):\n",
    "        time_begin = time.time()\n",
    "        \n",
    "        # initial features\n",
    "        feat = {}\n",
    "        for key, value in node_symbols.items():\n",
    "            if key in emb_symbols.keys():\n",
    "                feat['{}_feat'.format(value)] = data[emb_symbols[key]]\n",
    "            else:\n",
    "                feat['{}_feat'.format(value)] = data['{}_feat'.format(value)]\n",
    "        feat = Data.from_dict(feat)\n",
    "        \n",
    "        # training\n",
    "        pos_edge = data.train_edge_index[cv].clone()\n",
    "        ret_feat, train_loss, metrics = train(\n",
    "            feat, pos_edge, node_symbols, used[cv], data.internal_src_index, data.internal_tar_index,\n",
    "            data.external_src_index, data.external_tar_index, conf\n",
    "        )\n",
    "        auroc, auprc, acc = metrics['AUROC'], metrics['AUPRC'], metrics['ACC']\n",
    "        train_records[cv].append([auroc, auprc, acc])\n",
    "        train_losses[cv].append(train_loss)\n",
    "        if epoch%conf['verbose']==conf['verbose']-1 or epoch==0:\n",
    "            print(\"Train >> \", end='')\n",
    "            if epoch==0:\n",
    "                print('EPOCH:{:3d}  AUROC:{:0.4f}  AUPRC:{:0.4f} ACC:{:0.4f}  TIME:{:0.2f}'.format(\n",
    "                    epoch+1, auroc, auprc, acc, (time.time()-time_begin)))\n",
    "            else:\n",
    "                print('EPOCH:{:3d}  VALID_LOSS:{:0.4f}  AUROC:{:0.4f}  AUPRC:{:0.4f} ACC:{:0.4f}  TIME:{:0.2f}'.format(\n",
    "                    epoch+1, train_loss, auroc, auprc, acc, (time.time()-time_begin)))\n",
    "\n",
    "        \"\"\" Valid Model (external) \"\"\"\n",
    "        val_loss, val_metrics = valid(\n",
    "            ret_feat, data.valid_edge_index[cv], data.valid_neg_edge_index[cv], node_symbols, emb_symbols,\n",
    "            data.internal_src_index, data.internal_tar_index, conf\n",
    "        )   \n",
    "        val_auroc, val_auprc, val_acc = val_metrics['AUROC'], val_metrics['AUPRC'], val_metrics['ACC']\n",
    "        valid_records[cv].append([val_auroc, val_auprc, val_acc])\n",
    "        valid_losses[cv].append(val_loss)\n",
    "\n",
    "        if epoch%conf['verbose']==conf['verbose']-1 or epoch==0:\n",
    "            print(\"Valid >> \", end='')\n",
    "            if epoch==0:\n",
    "                print('EPOCH:{:3d}  AUROC:{:0.4f}  AUPRC:{:0.4f} ACC:{:0.4f}  TIME:{:0.2f}'.format(\n",
    "                    epoch+1, val_auroc, val_auprc, val_acc, (time.time()-time_begin)))\n",
    "            else:\n",
    "                print('EPOCH:{:3d}  VALID_LOSS:{:0.4f}  AUROC:{:0.4f}  AUPRC:{:0.4f} ACC:{:0.4f}  TIME:{:0.2f}'.format(\n",
    "                    epoch+1, val_loss, val_auroc, val_auprc, val_acc, (time.time()-time_begin)))\n",
    "\n",
    "        \"\"\" Test and Update \"\"\"\n",
    "        if val_auprc >= best_auprc:\n",
    "            best_auprc = val_auprc\n",
    "            print(\"\\t-- UPDATE BEST MODEL --\")\n",
    "            state = {\n",
    "                'cv': cv,\n",
    "                'epoch': epoch_num,\n",
    "                'net': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            test_loss, test_metrics = test(\n",
    "                ret_feat, data.test_edge_index, data.test_neg_edge_index, node_symbols, emb_symbols,\n",
    "                data.internal_src_index, data.internal_tar_index, conf\n",
    "            )\n",
    "            test_auroc, test_auprc, test_acc = test_metrics['AUROC'], test_metrics['AUPRC'], test_metrics['ACC']\n",
    "            best_records[cv].append(test_metrics)\n",
    "    \n",
    "        epoch_num += 1\n",
    "        \n",
    "        \"\"\"\n",
    "        if epoch%100==0 or epoch==0:\n",
    "            print(\"#\"*50)\n",
    "            print(\"Plot Figures start\")\n",
    "            print(\"Plot Figures finish\")\n",
    "            print(\"#\"*50)\n",
    "            plot_global_PCA(ret_feat, node_symbols, figname='global_PCA_epoch{}.jpg'.format(epoch))\n",
    "            plot_local_PCA(ret_feat['d_feat'], nodes['drug'], did_to_clst, figname='local_PCA_epoch{}.jpg'.format(epoch))\n",
    "            plot_local_TSNE(ret_feat['d_feat'], nodes['drug'], did_to_clst, figname='local_PCA_epoch{}.jpg'.format(epoch))\n",
    "        \"\"\"\n",
    "        \n",
    "    else:\n",
    "        \"\"\" Save model \"\"\"\n",
    "        best_states[cv] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(1, epoch_num+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aurocs, train_auprcs, train_accs = [], [], []\n",
    "for i in range(conf['cv']):\n",
    "    train_auroc, train_auprc, train_acc = np.array(train_records[i]).T\n",
    "    train_aurocs.append(train_auroc)\n",
    "    train_auprcs.append(train_auprc)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "valid_aurocs, valid_auprcs, valid_accs = [], [], []\n",
    "for i in range(conf['cv']):\n",
    "    valid_auroc, valid_auprc, valid_acc = np.array(valid_records[i]).T\n",
    "    valid_aurocs.append(valid_auroc)\n",
    "    valid_auprcs.append(valid_auprc)\n",
    "    valid_accs.append(valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "sig_digit = 4\n",
    "for cv, (auroc, auprc, acc) in enumerate(zip(valid_aurocs, valid_auprcs, valid_accs)):\n",
    "    dic['{}'.format(cv)] = [\n",
    "        round(max(auroc), sig_digit), round(max(auprc), sig_digit), round(max(acc), sig_digit)\n",
    "    ]\n",
    "df = pd.DataFrame(dic).transpose()\n",
    "df = df.rename(columns={0:'AUROC', 1:'AUPRC', 2:'ACC'})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result of cross-validation\n",
    "ave, std = calc_average_and_std(valid_aurocs)\n",
    "print(\"Valid AUROC: {:0.4f} +/- {:0.4f}\".format(ave, std))\n",
    "\n",
    "ave, std = calc_average_and_std(valid_auprcs)\n",
    "print(\"Valid AUPRC: {:0.4f} +/- {:0.4f}\".format(ave, std))\n",
    "\n",
    "ave, std = calc_average_and_std(valid_accs)\n",
    "print(\"Valid ACC:   {:0.4f} +/- {:0.4f}\".format(ave, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUORC\n",
    "for cv in range(conf['cv']):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(epochs, train_aurocs[cv], label='train')\n",
    "    plt.plot(epochs, valid_aurocs[cv], label='valid')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('AUROC')\n",
    "    plt.title('CV: {}'.format(cv))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUPRC\n",
    "for cv in range(conf['cv']):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(epochs, train_auprcs[cv], label='train')\n",
    "    plt.plot(epochs, valid_auprcs[cv], label='valid')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('AUPRC')\n",
    "    plt.title('CV: {}'.format(cv))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ACC\n",
    "for cv in range(conf['cv']):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(epochs, train_accs[cv], label='train')\n",
    "    plt.plot(epochs, valid_accs[cv], label='valid')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('ACC')\n",
    "    plt.title('CV: {}'.format(cv))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "for cv in range(conf['cv']):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(epochs, train_losses[cv], label='train')\n",
    "    plt.plot(epochs, valid_losses[cv], label='valid')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('CV: {}'.format(cv))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "for cv in range(conf['cv']):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(epochs[10:], train_losses[cv][10:], label='train')\n",
    "    plt.plot(epochs[10:], valid_losses[cv][10:], label='valid')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('CV: {}'.format(cv))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test result & Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(state, txt):\n",
    "    dt = datetime.datetime.now()\n",
    "    stamp = dt.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    torch.save(state['net'], '{}_{}_ModelStates.pth'.format(stamp, txt))\n",
    "    torch.save(state['optimizer'], '{}_{}_OptimizerStates.pth'.format(stamp, txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aurocs, auprcs, accs = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "for i in range(conf['cv']):\n",
    "    for record in best_records[i]:\n",
    "        aurocs[i].append(record['AUROC'])\n",
    "        auprcs[i].append(record['AUPRC'])\n",
    "        accs[i].append(record['ACC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_idx = np.argmax(np.array([auprcs[i][-1] for i in range(conf['cv'])]))\n",
    "print(\"Best cv: {}\".format(best_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUROC: {:.4f}'.format(np.array([aurocs[i][-1] for i in range(conf['cv'])])[best_idx]))\n",
    "print('AUPRC: {:.4f}'.format(np.array([auprcs[i][-1] for i in range(conf['cv'])])[best_idx]))\n",
    "print('ACC  : {:.4f}'.format(np.array([accs[i][-1] for i in range(conf['cv'])])[best_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf['save_model']:\n",
    "    save_models(best_states[best_idx], 'MyModel_All_wCV')\n",
    "    for key, val in node_symbols.items():\n",
    "        np.save('embeddings/{}_feat.npy'.format(val), ret_feat['{}_feat'.format(val)].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_edge_prediction(path, data, model, device):\n",
    "    model.eval()\n",
    "    mat = np.loadtxt(path+'/Edge/dz.txt')\n",
    "    \n",
    "    # unknown drug-disease pairs\n",
    "    unknown_pairs = []\n",
    "    for i in range(data.n_drug):\n",
    "        for j in range(data.n_diz):\n",
    "            if mat[i][j] != 1:\n",
    "                unknown_pairs.append([i,j])\n",
    "    unknown_pairs = Tensor(np.array(unknown_pairs)).long().T\n",
    "    unknown_pairs = unknown_pairs.to(device)\n",
    "    unknown_pairs[1] += data.n_drug\n",
    "    \n",
    "    # evaluate all unknown pairs\n",
    "    z_drug = data.d_feat.clone()\n",
    "    z_diz = data.z_feat.clone()\n",
    "    score = model.decoder(z_drug, z_diz, unknown_pairs).detach().cpu().numpy()\n",
    "    unknown_pairs[1] -= data.n_drug\n",
    "    score = np.concatenate([unknown_pairs.T.detach().cpu().numpy(), score], axis=1)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_DR_pairs(path, score, num_results=10):\n",
    "    # sort by link-probability\n",
    "    sorted_score = score[np.argsort(score[:, 2])][::-1]\n",
    "    \n",
    "    # each node name\n",
    "    drug_name = load_txt(path+'/Node/drug.txt')\n",
    "    diz_name = load_txt(path+'/Node/disease.txt')\n",
    "    \n",
    "    print(f'{\"Drug\":20}  {\"Disease\":8}    {\"Score\":5}')\n",
    "    print('_'*40+'\\n')\n",
    "    for i in range(num_results):\n",
    "        [i,j,v] = sorted_score[i].tolist()\n",
    "        print(f'{drug_name[int(i)]:20}  {diz_name[int(j)]:8} >> {v:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
